[
    {
        "index": 1,
        "type": "MultipleChoice",
        "question": "⼈⼯智能、机器学习和深度学习三者之间的关系是(  )",
        "options": {
            "A": "深度学习是机器学习的⼀种实现⽅法",
            "B": "三个不同的领域,分别独⽴但互有交叉",
            "C": "计算机视觉、语⾳识别和⾃然语⾔处理都是深度学习的具体应⽤,也都属于机器学习和⼈⼯智能的范畴",
            "D": "⼈⼯神经⽹络是深度学习最具代表性的⼀种⽅法,它与⼈⼯智能和机器学习⽆关"
        },
        "answers": [
            "A",
            "C"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>人工智能是一个很大的范畴,它指努力将通常由人类完成的智力任务自动化。机器学习是人工智能的一种方法,专门用于解决分类、回归、检测等任务。深度学习是机器学习的一种具体实现,它使用多层迭代的方式解决机器学习的具体应用。</p></div>"
    },
    {
        "index": 2,
        "type": "MultipleChoice",
        "question": "深度学习已经在很多任务上获得了重⼤突破,以下属于这⼀类的包括:(\n)。",
        "options": {
            "A": "接近⼈类⽔平的图像分类",
            "B": "更好的机器翻译",
            "C": "接近⼈类⽔平的逻辑判断与推理",
            "D": "⾃动⽂本到语⾳的转换"
        },
        "answers": [
            "A",
            "B",
            "D"
        ],
        "explanation": "深度学习目前主要在面向视觉和听觉的感知任务上有较大的突破,在自然语言方面也有一定的成果。但对于形式推理及通用人工智能方面依然不足。"
    },
    {
        "index": 3,
        "type": "SingleChoice",
        "question": "⼈⼯智能的第⼆次寒冬是由于( )并没有完成它所被期望任务。",
        "options": {
            "A": "专家系统",
            "B": "⽀持向量机",
            "C": "多层感知机",
            "D": "符号主义"
        },
        "answers": [
            "A"
        ],
        "explanation": "20世纪80年代,一种新的符号主义人工智能——专家系统,开始在大公司中受到追捧。但专家系统的维护费用变得很高,并且难以扩展,应用面有限。 "
    },
    {
        "index": 4,
        "type": "SingleChoice",
        "question": "⼈⼯智能的第⼀次寒冬是由于( )并没有完成它所被期望任务。",
        "options": {
            "A": "专家系统",
            "B": "⽀持向量机",
            "C": "多层感知机",
            "D": "符号主义"
        },
        "answers": [
            "D"
        ],
        "explanation": "20世纪60年代,符号主义横空出世,以马文·闵斯基为代表的先驱者们,一度认为这预示着人工智能的未来。但在随后的几年这种过高的期望并没有实现,甚至到2020年的今天,这一目标仍然十分遥远,遥远到我们无法预测需要多长时间才能实现。"
    },
    {
        "index": 5,
        "type": "SingleChoice",
        "question": "到⽬前为⽌,⼈⼯智能已经经历了( )次寒冬,这⼏次寒冬都给⼈⼯智能的发展带来了⼏乎毁灭性的影响。",
        "options": {
            "A": "⼀次",
            "B": "⼆次",
            "C": "三次",
            "D": "四次"
        },
        "answers": [
            "B"
        ],
        "explanation": "至今人工智能已经经历了两次寒冬,第一次寒冬发生在19世纪80年代初主要原因是对符号主义期待过高;第二次寒冬发生在20世纪90年代,主要原因是专家系统的成本太高且应用面太窄。"
    },
    {
        "index": 6,
        "type": "SingleChoice",
        "question": "⼈⼯智能诞⽣于20世纪50年代的(\n)。",
        "options": {
            "A": "博鳌论坛",
            "B": "达特茅斯会议",
            "C": "世界⼈⼯智能⼤会",
            "D": "MIT⼈⼯智能研究⼩组内部会议"
        },
        "answers": [
            "B"
        ],
        "explanation": "1956年8月,在美国汉诺斯小镇宁静的达特茅斯学院中,约翰·麦卡锡(John McCarthy)、马文·闵斯基(Marvin Minsky,人工智能与认知学专家)、克劳德·香农(Claude Shannon,信息论的创始人)、艾伦·纽厄尔(Allen Newell,计算机科学家)、赫伯特·西蒙(Herbert Simon,诺贝尔经济学奖得主)等科学家正聚在一起,讨论着一个完全不食人间烟火的主题:用机器来模仿人类学习以及其他方面的智能。会议足足开了两个月的时间,虽然大家没有达成普遍的共识,但是却为会议讨论的内容起了一个名字:人工智能。因此,1956年也就成为了人工智能元年。"
    },
    {
        "index": 7,
        "type": "MultipleChoice",
        "question": "在最近⼏年的ImageNet⼤规模视觉识别挑战赛(ILSVRC)中,性能最好的模型属于()。",
        "options": {
            "A": "Mark I 感知机",
            "B": "时空图模型",
            "C": "⽀持向量机",
            "D": "卷积神经⽹络"
        },
        "answers": [
            "D"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>近年来卷积神经网络在包括Imagenet大规模视觉识别挑战赛中展露头角,获得了举世瞩目的成绩,包括Alexnet,VGG,GoogLeNet、ResNet在内的的很多CNN不断刷新各种竞赛的最高分。</p></div>"
    },
    {
        "index": 8,
        "type": "SingleChoice",
        "question": "以下常⻅带标注的数据集中,样本数据量最⼤的是()。",
        "options": {
            "A": "Pascal VOC",
            "B": "Caltech101",
            "C": "MSCOCO",
            "D": "LabelMe",
            "E": "ImageNet"
        },
        "answers": [
            "E"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>Caltect101,9K;PascalVOC2012Trainval,17K;\nLabelMe,37K;\nSun, 131K; MSCOCO, 328K; ImageNet, 14M;其中ImageNet中抽取137W数据,1000个类,作为ILSVRC竞赛数据。</p></div>"
    },
    {
        "index": 9,
        "type": "Judgement",
        "question": "在处理视频数据时,3D卷积和2D卷积最⼤的区别时直接编码学习空间信息特征。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "在处理视频数据时,3D卷积和2D卷积最大的区别时直接编码学习时间信息特征。\n\n \n  \n\n"
    },
    {
        "index": 10,
        "type": "Judgement",
        "question": "在对视频内容进⾏特征提取的时候,⼀般⽆法实现对⾼分辨率样本和低分辨率样本的同时学习,因为不同分辨率是⽆法进⾏融合的。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "对于不同分辨率的样本,只要它们尺度是相同的,或者可以通过一定的空间变换实现尺度的统一,都可以实现特征融合。\n"
    },
    {
        "index": 11,
        "type": "Judgement",
        "question": "在视频分类数据集中,最常⻅的任务是对视频中的⾏为进⾏分类,也就是说视频分类是描述视频中的⾏为是在做什么的任务。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "在视频分类任务中,最常见的任务就是判断视频中的行为是在做什么,例如:射箭、骑车、奔跑。但这并不是视频分类的全部描述,例如:判断视频所描述的场景是哪里,判断视频所描述的地点等也是视频分类的常见任务。\n"
    },
    {
        "index": 12,
        "type": "Judgement",
        "question": "在计算机和互联⽹出现之前,整个⼈⼯智能领域缺少数据,也缺乏计算资源,唯⼀值得称道的只有优秀的机器学习算法。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "在计算机视觉出现的起源阶段,我们不但没有数据,没有互联网,也没有足够的算力;同样,也几乎没有任何机器学习算法的知识和积累;拥有的仅仅只是少量的统计学习的知识。\n"
    },
    {
        "index": 13,
        "type": "Judgement",
        "question": "爱因斯坦复杂问题曾经有过⼀段定义,他说到“对于⼀个复杂的问题,解决问题⽐理解问题内在的含义更加重⽤,因为解决问题需要更多的想象⼒,这也标志着科学的真正进步。”()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "对一个问题的简单阐述往往比解决该问题更重要,因为它需要创造性的想象力,它标志着科学的真正进步。——Albert Einstein, 1921\n"
    },
    {
        "index": 14,
        "type": "Judgement",
        "question": "OCR⼜称为光学字符识别,它指将电⼦设备上的字符,通过某种字符识别⽅法将其转换成计算机⽂字的过程。整个过程主要是⾃然语⾔处理的范畴,与计算机视觉⽆关。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "OCR的前期工作是从图像中去识别文字,这个过程实际上就是将视觉内容向文本转换的过程,所以也属于计算机视觉应用的范畴。\n"
    },
    {
        "index": 15,
        "type": "MultipleChoice",
        "question": "在处理基于视觉内容的识别任务中,下列属于深度学习中常⻅特征的包括()。",
        "options": {
            "A": "单帧静态的外观信息",
            "B": "多帧光流信息",
            "C": "多帧运动轨迹",
            "D": "单帧HoG信息"
        },
        "answers": [
            "A",
            "B",
            "C"
        ],
        "explanation": "在处理视频片段的深度学习模型中,常见的特征包括静态的单帧特征和包含时间信息的多帧特征。单帧特征主要提取对象的外观信息,多帧特征主要包括像素的运动轨迹和位移(光流)两种信息。选项D所描述的HoG特征主要是指对象的边缘梯度信息,一般在传统方法中使用,当然也可以作为一种辅助特征融合到深度特征中。"
    },
    {
        "index": 16,
        "type": "MultipleChoice",
        "question": "在对多帧进⾏特征融合的时候,常⻅的融合⽅式包括()。",
        "options": {
            "A": "后期融合Late\nFusion",
            "B": "中期融合Middle Fusion",
            "C": "早期融合Early\nFusion",
            "D": "慢融合Slow\nFusion",
            "E": "快融合Fast Fusion"
        },
        "answers": [
            "A",
            "C",
            "D"
        ],
        "explanation": "理论上任何融合方式,只要可以实现都是可行的。但在本题中所提到的C中期融合是很难去实现,当然这种方式也可以理解为一种瓶颈设计,只是目前还没有相关的文献提出;E选项提出的快融合可以理解为早期融合,即在视频帧输入到神经网络的开始阶段就进行融合,但一般不这么描述。所以,正确答案为ACD。 "
    },
    {
        "index": 17,
        "type": "MultipleChoice",
        "question": "在处理基于视觉内容的识别任务中,下列属于传统⽅法中常⻅特征的包括()。",
        "options": {
            "A": "费舍尔向量",
            "B": "CNN特征",
            "C": "运动轨迹",
            "D": "HoG⽅向梯度直⽅图",
            "E": "视觉词袋",
            "F": "光流信息"
        },
        "answers": [
            "C",
            "D",
            "F"
        ],
        "explanation": "CDF都是传统方法中常见的特征,其中光流信息和运动轨迹在基于深度学习的方法中依然很常用;A费尔舍向量和E词袋模型属于特征融合算法;B,CNN特征是最基本深度模型特征"
    },
    {
        "index": 18,
        "type": "MultipleChoice",
        "question": "在视频应⽤中,下列哪些问题属于常⻅的难点?()",
        "options": {
            "A": "外观变化",
            "B": "视⻆变换",
            "C": "背景变换",
            "D": "光照变化"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "在当前基于视频的应用中,有许多的难点,包括:外观变化、视角变换、背景变换、光照变化、遮挡、景深的变化(尺度)、分辨率的不足。"
    },
    {
        "index": 19,
        "type": "MultipleChoice",
        "question": "以下任务,哪些属于基于视频理解的常⻅应⽤?()",
        "options": {
            "A": "⾮法信息检测",
            "B": "视频格式转换",
            "C": "特定⼈物识别",
            "D": "智慧交通"
        },
        "answers": [
            "A",
            "C",
            "D"
        ],
        "explanation": "当前,很多应用都是基于视频的视觉理解,这些应用不断地改善这我们的日常生活,例如智慧交通、智慧医疗、智慧教育、特定人物/目标识别、非法信息检测、行人检测、自动驾驶、自动机器人导航等。"
    },
    {
        "index": 20,
        "type": "MultipleChoice",
        "question": "以下特点,哪些是⽹络视频具有的特点。()",
        "options": {
            "A": "数据量⼤、占⽐⾼",
            "B": "增速快",
            "C": "信息量⼤",
            "D": "传播快",
            "E": "实时性、鲁棒性、⾼度集成"
        },
        "answers": [
            "A",
            "B",
            "D",
            "E"
        ],
        "explanation": "视频是当前互联网中最主要的数据形式,它具有数据量、占比高,增速快,传播快,实时性、鲁棒性和高度集成等特点。"
    },
    {
        "index": 21,
        "type": "MultipleChoice",
        "question": "以下名词,属于数据集的是哪⼏项?()",
        "options": {
            "A": "Caltech101",
            "B": "ResNet",
            "C": "ImageNet",
            "D": "Pascal VOC",
            "E": "MSCOCO"
        },
        "answers": [
            "A",
            "C",
            "D",
            "E"
        ],
        "explanation": "以上名词中,B选项ResNet是一种卷积神经网络模型,由何凯明等人发明;Caltech101, ImageNet, Pascal VOC, MSCOCO都是著名的数据集,这些数据集可以用来评估图像分类,目标检测、图像分割,视觉内容问答等多种任务。 "
    },
    {
        "index": 22,
        "type": "MultipleChoice",
        "question": "以下技术,哪⼀个算法可以⽤来实现⽬标识别?()",
        "options": {
            "A": "Normalized Cut",
            "B": "尺度不变特征变换SIFT",
            "C": "梯度直⽅图HoG",
            "D": "部件模型DPM"
        },
        "answers": [
            "B",
            "C",
            "D"
        ],
        "explanation": "Normlized Cut是一种图像分割算法,它可以实现将不同物体/区域的像素用不同的色彩标注出来;尺度不变特征变换SIFT是一种特征匹配算法,通过计算两幅图像之间匹配特征的数量实现目标的识别;HoG和DPM是两种基于梯度的特征提取算法,通过特征的提取和对比,同样可以实现目标的识别。"
    },
    {
        "index": 23,
        "type": "MultipleChoice",
        "question": "以下属于计算机视觉的具体应⽤领域的包括()。",
        "options": {
            "A": "⽬标检测",
            "B": "场景理解",
            "C": "三维重建",
            "D": "⾏⼈识别",
            "E": "⽬标跟踪"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D",
            "E"
        ],
        "explanation": "计算机视觉的应用涉及面非常广,包括目标检测、分类、场景理解、语义/实例分割、三维重建、目标跟踪,行人姿态估计、行为识别、视觉内容问答等。"
    },
    {
        "index": 24,
        "type": "MultipleChoice",
        "question": "计算机视觉是⼀个跨学科的领域,它涉及到诸多的领域,包括()。",
        "options": {
            "A": "认知科学",
            "B": "机器⼈学",
            "C": "图像处理",
            "D": "物理学"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "计算机视觉是一个涉及很多学科的领域,为了实现计算机视觉的应用,可能需要掌握很多的领域,包括生物学、心理学、物理学、工程学、数学、计算机科学等。以上选项中,认知科学是心理学的范畴,机器人学是工程学的领域,图像处理是计算机领域的知识,而物理学中的光学知识是计算机视觉数据的来源。"
    },
    {
        "index": 25,
        "type": "SingleChoice",
        "question": "给定⼀个视频⽚段和⼀个带空格的语句,计算机通过训练好的模型在句⼦中的空⽩部分填⼊正确的词汇。这种任务⼀般称为()。",
        "options": {
            "A": "视频分类",
            "B": "⾏为识别",
            "C": "基于视觉内容的问答",
            "D": "视频检索"
        },
        "answers": [
            "C"
        ],
        "explanation": " 基于视觉内容的填空是基于视觉内容问答的一种简化形式,它要求计算机对给定好的带空格的语句进行填空。更完整的基于视觉内容的问答是一问一答的形式,这种没有提示的问答相比填空要更难一些。"
    },
    {
        "index": 26,
        "type": "SingleChoice",
        "question": "()技术是⽬前三维重建、⽆⼈机路线规划、机器⼈路线规划的基础。",
        "options": {
            "A": "空间⾦字塔",
            "B": "点云",
            "C": "虚拟现实",
            "D": "图像分割"
        },
        "answers": [
            "B"
        ],
        "explanation": "点云是指目标表面特性的海量点集合,是物体表面最具代表性的特征的采样点。"
    },
    {
        "index": 27,
        "type": "SingleChoice",
        "question": "梯度⽅向直⽅图HoG和可变形部件模型DPM是计算机视觉的经典模型,它们是专⻔针对()应⽤⽽设计,并获得了巨⼤的成功。",
        "options": {
            "A": "⼈脸识别",
            "B": "⾏⼈识别",
            "C": "⻋牌识别",
            "D": "通⽤对象识别"
        },
        "answers": [
            "B"
        ],
        "explanation": "梯度方向直方图HoG和可变形部件模型DPM最初的设计是面向行人的特征提取,它们可以获得非常鲁棒的关于人体的梯度特征,被广泛应用在行人识别和行人再识别等任务中。当然,它们也可以用于其他对象的识别。"
    },
    {
        "index": 28,
        "type": "SingleChoice",
        "question": "解决对象识别的三个基础元素是()。",
        "options": {
            "A": "数据、知识、GPU",
            "B": "算法、知识、GPU",
            "C": "数据、算法、学习",
            "D": "数据、学习、知识"
        },
        "answers": [
            "D"
        ],
        "explanation": "解决对象识别的三个基础元素是数据、学习、知识。"
    },
    {
        "index": 29,
        "type": "SingleChoice",
        "question": "世界上第⼀个计算机视觉项⽬成⽴于1966年的()。",
        "options": {
            "A": "斯坦福⼤学",
            "B": "哈佛⼤学",
            "C": "麻省理⼯学院",
            "D": "牛津⼤学",
            "E": "加州理⼯⼤学"
        },
        "answers": [
            "C"
        ],
        "explanation": " 1966年7月7日,在麻省理工学院第一个视觉项目\"The sumer vision Project\"建立,它致力于研究模式识别的一个子问题——视觉。"
    },
    {
        "index": 30,
        "type": "SingleChoice",
        "question": "Hubel和Wiesel在1958年的( )视觉⽪层实验中,⾸次观察到视觉初级⽪层的神经元对移动的边缘刺激敏感,发现了视功能柱结构,为计算机视觉神经研究奠定了重要的基础。",
        "options": {
            "A": "蜜蜂",
            "B": "章鱼",
            "C": "猫",
            "D": "⼈",
            "E": "狗"
        },
        "answers": [
            "C"
        ],
        "explanation": "Hubel和Wiesel在1958年的猫视觉皮层实验中,首次观察到视觉初级皮层的神经元对移动的边缘刺激敏感,并定义了简单和复杂细胞,发现了视功能柱结构。此项工作为视觉神经研究奠定了重要的基础,两人在1981年共享了诺贝尔生理学或医学奖,以表彰他们在“视觉系统信息加工”的重要贡献。"
    },
    {
        "index": 31,
        "type": "SingleChoice",
        "question": "视觉起源于(\n)⽣物⼤爆发,在这次⽣物⼤爆发⾥视觉的进化是突然的,动物⾯临着进化和死亡的抉择。",
        "options": {
            "A": "奥陶纪",
            "B": "寒武纪",
            "C": "侏罗纪",
            "D": "⽩垩纪\nParker"
        },
        "answers": [
            "B"
        ],
        "explanation": "寒武纪大爆发是由视觉的突然进化引发的,这引发了一场关于进化的军备竞赛,这个时期的动物要么进化,要么死亡。——AndrewParker"
    },
    {
        "index": 32,
        "type": "SingleChoice",
        "question": "⼈脸识别是⼴为⼈知的应⽤,它属于以下哪个任务的范畴?(\n)",
        "options": {
            "A": "⽬标识别",
            "B": "三维重建",
            "C": "图像分割",
            "D": "视频内容问答"
        },
        "answers": [
            "A"
        ],
        "explanation": "完善的人脸识别系统涉及到很多的领域,特别是今天越来越安全的人脸识别系统,但标准人脸识别系统属于目标识别(分类/验证)。当然,今天的人脸识别系统越来越复杂,可能还涉及到三维重建,行为识别等功能。"
    },
    {
        "index": 33,
        "type": "MultipleChoice",
        "question": "下列数据源对计算机视觉任务具有较⼤帮助的包括:( )。",
        "options": {
            "A": "Flickr",
            "B": "维基百科",
            "C": "YouTube",
            "D": "Imagenet"
        },
        "answers": [
            "A",
            "C",
            "D"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>在过去20年里,数据领域最大的变革来源于互联网的兴起,它使得收集与分发用于机器学习的超大型数据集变得可行。其中Flickr、YouTube是面向视觉领域的数据源;维基百科是面向自然语言处理的关键数据库;Imagenet、Pascal VOC等数据库推动了分类、检测等多个视觉任务的发展。</p></div>"
    },
    {
        "index": 34,
        "type": "SingleChoice",
        "question": "以下程序开发语⾔中,在深度学习中最流⾏的是( )。",
        "options": {
            "A": "Html5+CSS3",
            "B": "Javascript语⾔",
            "C": "Python语⾔",
            "D": "C语⾔",
            "E": "Java语⾔"
        },
        "answers": [
            "C"
        ],
        "explanation": "从某种程度上说,应用开放与程序语言并没有太直接的关系,但是由于生态圈和语言的易用性考虑,对于程序设计语言的选择依然具有偏向性。在人工智能领域,底层的开发涉及高性能和并行计算,因此最常见的是基于CUDA库的C++语言;而在面向用户接口部分更多的需要的是简便性,即能实现敏捷开发,因此Python语言是最常见的开发语言。在人工智能领域,诸如Java, Javascript, Matlab等开发语言也有一些开发库,但应用面相对要窄很多。 "
    },
    {
        "index": 35,
        "type": "SingleChoice",
        "question": "关于深度学习的原理,下列描述正确的⼀项是( )",
        "options": {
            "A": "学习是指找到⼀种最优的模型,使其能够较好地学到数据的内在知识",
            "B": "⼀个模型的损失是指输⼊样本的真实值和基于样本获得的预测值之间的距离,它是在训练过程种需要最⼤化的量",
            "C": "优化神经⽹络⽆法使⽤解析法,但是可以通过求导的链式法则来获得梯度函数,并利⽤随机梯度下降等⽅法来实现模型的优化",
            "D": "随机梯度下降算法能够有效解决局部最优的问题"
        },
        "answers": [
            "C"
        ],
        "explanation": "学习是指找到一组模型参数,使得在给定的训练数据样本和对应目标值上的损失函数最小化。学习的过程:随机选取包含数据样本及其目标值的批量,并计算批量相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动(距离由学习率指定)。整个学习过程之所以能够实现,是因为神经网络是一系列可微分的张量运算,因此可以利用求导的链式法则来得到梯度函数,这个函数将当前参数和当前数据批量映射为一个梯度值。损失是在训练过程中需要最小化的量,它可以衡量当前任务是否已经成功解决。优化器是使用损失梯度更新参数的具体方式,例如RMSProp、带动量的随机梯度下降(SGD)等 "
    },
    {
        "index": 36,
        "type": "SingleChoice",
        "question": "随机梯度下降算法是利⽤梯度的微⼩变化来优化损失函数的⼀种⽅法,下列哪种随机梯度下降算法最适合于⾯向⼤数据的神经⽹络。( )",
        "options": {
            "A": "真随机梯度下降,每次只迭代⼀个样本",
            "B": "全样本梯度下降,每次迭代都在所有数据上运⾏",
            "C": "随机随机梯度下降,每次迭代时随机选取⼀定数量的样本进⾏处理",
            "D": "⼩批量随机梯度下降,每次迭代都在固定量(通常都不⼤)的样本上进⾏"
        },
        "answers": [
            "D"
        ],
        "explanation": "随机梯度下降有多个变种,除了小批量随机梯度下降mini-batch SGD特别适合于深度学习外,还包括真SGD和全样本SGD。每次迭代只取一个样本,称为真SGD。该方法资源浪费大,且随机性过大,影响更新的准确性。每次迭代都在所有数据上运行,称为全样本SGD。该方法每次更新更加准确,但计算代价高得多,且对于大数据,GPU设备可能无法容纳。 "
    },
    {
        "index": 37,
        "type": "SingleChoice",
        "question": "关于神经⽹络中权重更新的描述,说法正确的是( )。",
        "options": {
            "A": "权重更新的⽅向取决于预测值与期望值的差值",
            "B": "权重更新的⽅向总是沿着梯度的正⽅向",
            "C": "权重更新的⽅向是沿着梯度的反⽅向,以减⼩损失",
            "D": "权重更新的⼤⼩与⽹络参数的数量⽆关"
        },
        "answers": [
            "C"
        ],
        "explanation": "选项A,虽然预测值与期望值的差值会影响损失函数的值,但权重更新的方向并不直接取决于这个差值本身,而是取决于损失函数对权重的梯度。选项B显然错误,权重更新的方向沿着梯度的反方向,因此选项C正确。选项D不完全准确,权重更新的大小通常由学习率来控制,其更新的基数是由误差决定,误差和学习率的选择可能都会受到网络参数数量、损失函数的性质以及训练数据的影响,进而影响权重更新的大小。"
    },
    {
        "index": 38,
        "type": "SingleChoice",
        "question": "下列公式能够正确表⽰函数f在距离起始点W0附近的点W1的梯度的是( )。",
        "options": {
            "A": "W1=w0−step*gradient(f)(w0)",
            "B": "W1=W0−step*gradient(f)(W0)",
            "C": "W1=step*gradient(f)(w0)",
            "D": "W1=step*gradient(f)(W0)"
        },
        "answers": [
            "B"
        ],
        "explanation": "假设W的当前值为W0,则f在W0点的导数是一个张量gradient(f)(W0),其形状与WW相同。对于张量W的函数f(W),可以通过将WW向梯度的反方向移动来减小f(W),例如:W1=W0−step*gradient(f)(W0)。其中,step 是一个很小的比例因子。也就是说,沿着曲率的反方向移动,直观上看在曲线上的位置会更低。其中,比例因子 step 是必需的,与pp点导数相似,gradient(f)(W0)只是在W0附近曲率的近似值,不能离W0太远。值得注意的是:大写W表示的是张量,而小写w表示的是标量。"
    },
    {
        "index": 39,
        "type": "SingleChoice",
        "question": "若存在⼀个连续的光滑函数f(),若存在点M,当点M的导数为w<0时,x在点M附近的微⼩变化将导致函数f() ( )。",
        "options": {
            "A": "不变",
            "B": "减⼩",
            "C": "增⼤",
            "D": "⽆法确定"
        },
        "answers": [
            "B"
        ],
        "explanation": "若w时,在p点导数,则如果a是负值,则x在p点附近的微小变化将导致f(x)减小;如果a是正值,则x在p点附近的微小变化将导致f(x)增大。a的绝对值的大小(导数大小),表示增大或减小的速度快慢。"
    },
    {
        "index": 40,
        "type": "SingleChoice",
        "question": "在对神经⽹络的权重矩阵进⾏初始化时,通常会将这些权重初始化为( )。",
        "options": {
            "A": "全0的值",
            "B": "全1的值",
            "C": "很⼩的随机值",
            "D": "很⼤的随机值",
            "E": "任意随机值"
        },
        "answers": [
            "C"
        ],
        "explanation": "随机初始化算法将权重矩阵初始化为一些很小的随机值。此时的值不具任何意义,但这是训练的起点。此后,这些权重会根据反馈信号逐渐进行调节,使其能够更好地反映训练数据的内在特征,这个过程就称为训练。选项A,如果将权重矩阵初始化为全0的值,那么在反向传播时,所有的神经元将会接收到相同的梯度,从而导致它们在更新权重时都做出相同的改变。这会导致网络的对称性,即网络无法学习不同的特征。选项B,全1的值与全0的值类似,也会导致网络在训练时的对称性;此外,全1值和较大的随机值(选项D)可能使得网络在初始阶段就处于激活函数的饱和区,从而导致梯度消失或梯度爆炸的问题,从而阻碍网络的训练。选项E中的任意随机值,其特性无法保证,在不限制的情况下,可能会遇到与D选项相同的问题,即网络在初始阶段可能处于激活函数的饱和区,导致梯度消失或梯度爆炸。因此,通常建议使用较小的随机值来初始化权重。"
    },
    {
        "index": 41,
        "type": "SingleChoice",
        "question": "神经⽹络的权重称为可训练参数,它反映了训练数据的内在特征,这些权重通常需要在( )时进⾏随机初始化。",
        "options": {
            "A": "第⼀次迭代训练开始前",
            "B": "每次迭代训练开始前",
            "C": "每次迭代训练结束后",
            "D": "最后⼀轮迭代训练结束后"
        },
        "answers": [
            "A"
        ],
        "explanation": "神经网络的权重被称为可训练参数,这意味着训练的目的是获得合适的权重。而在训练开始时,这些权重需要被初始化。随机初始化是一种比较好的方法,它使用随机算法将权重矩阵初始化为一些很小的随机值。"
    },
    {
        "index": 42,
        "type": "SingleChoice",
        "question": "基于多层感知机的神经⽹络使⽤( )层来构建⽹络模型。",
        "options": {
            "A": "卷积层",
            "B": "全连接层",
            "C": "池化层",
            "D": "半连接层"
        },
        "answers": [
            "B"
        ],
        "explanation": "在多层感知机(MLP)种,我们使用全连接层来构建神经网络,所谓全连接,其本质含义是指两个层之间的所有神经元都需要相互连接,我们可以使用 output=relu(dot(W, input)+ b) 来表示这种关系。但在全连接层后面,我们依然会使用激活函数来实现线性模型向非线性模型的转换。"
    },
    {
        "index": 43,
        "type": "SingleChoice",
        "question": "以下哪个选项与神经⽹络的优化⽆关?( )",
        "options": {
            "A": "梯度下降",
            "B": "反向传播",
            "C": "线性回归",
            "D": "批量梯度下降"
        },
        "answers": [
            "C"
        ],
        "explanation": " 在神经网络中,优化方法用于调整网络的参数(如权重和偏置)以最小化损失函数,从而改进模型的性能。梯度下降是一种常用的优化方法,它通过计算损失函数关于模型参数的梯度,并沿着梯度的反方向更新参数来最小化损失函数。梯度下降有多种变体,选项D的批量批量梯度下降就是其中的一种。选项B常与梯度下降一起使用,但它本身并不是一种优化方法,它用于更新梯度。选项C的线性回归是一种用于预测数值型数据的统计方法,与神经网络的优化无关。"
    },
    {
        "index": 44,
        "type": "SingleChoice",
        "question": "在神经⽹络训练开始时,权重通常如何初始化?( )",
        "options": {
            "A": "设置为1",
            "B": "设置为0",
            "C": "随机初始化",
            "D": "设置为⽆穷⼤"
        },
        "answers": [
            "C"
        ],
        "explanation": " 在神经网络训练开始时,权重的初始化对模型的训练效率和最终性能有着至关重要的影响。选项B中的全零初始化将权重全部初始化为0,那么在第一遍前向传播过程中,所有隐藏层神经元的激活函数值都相同,导致深层神经元可有可无,这一现象被称为对称权重现象。这会使得网络失去学习能力,因为不同神经元之间失去了区分性。选项A和D将权重初始化为1或无穷大都会导致网络在训练开始时表现出极端的行为。对于激活函数如sigmoid或tanh,过大的输入会导致梯度消失或饱和,从而使得网络难以通过梯度下降进行优化。选项C中的随机初始化是解决对称权重现象的有效方法。通过对每层的权重进行随机初始化,可以使得不同层的神经元之间有很好的区分性,该选项正确。"
    },
    {
        "index": 45,
        "type": "SingleChoice",
        "question": "感知机在执⾏训练时,有如下任务:(1)初始化权重;(2)更新权重;(3)输⼊数据,获得输出;(4)反复迭代;(5)计算预测值与真实值的误差。这些任务的正确执⾏顺序是( )。",
        "options": {
            "A": "1, 3, 5, 2, 4",
            "B": "1, 2, 3, 4, 5",
            "C": "3, 1, 2, 5, 4",
            "D": "3, 1, 5, 2, 4"
        },
        "answers": [
            "A"
        ],
        "explanation": null
    },
    {
        "index": 46,
        "type": "SingleChoice",
        "question": "反向传播算法是深度学习的核⼼,它利⽤( )作为反馈信号来对模型进⾏微调,从⽽降低损失值。",
        "options": {
            "A": "最后⼀层的权重",
            "B": "任意层的权重",
            "C": "预测值与真实值之间的距离",
            "D": "预测值"
        },
        "answers": [
            "C"
        ],
        "explanation": "深度学习的基本技巧是利用距离值作为反馈信号来对权重值进行微调,以降低当前示例对应的损失值。这种调节由优化器(optimizer)来完成,它实现了反向传播算法(backpropagation),这是深度学习的核心算法。"
    },
    {
        "index": 47,
        "type": "SingleChoice",
        "question": "神经⽹络中每层对输⼊数据所做的操作都保存在( )。",
        "options": {
            "A": "SQL Server或Oracle等关系型数据库中",
            "B": "输⼊样本的附加张量中",
            "C": "该层的权重中",
            "D": "⼀般不进⾏保存"
        },
        "answers": [
            "C"
        ],
        "explanation": "选项A是传统的数据库系统,用于存储结构化数据,而不是神经网络的结构或操作。在神经网络中,每层对输入数据所做的具体操作都保存在该层的权重(weight)中,其本质是一串数字。每层实现的变换由其权重来参数化。权重也被称为该层的参数,通常以张量形式进行保存。在实际操作中,这些权重参数会被以张量的形式进行存储,并保存在二进制文或文本文件中。这个被保存下来的文件就称为模型文件。在实际应用中,这些权重参数会被存储在二进制文件或文本文件中,这些文件通常被称为模型文件。模型文件是神经网络训练成果的具体体现,它包含了网络结构的信息以及各层的权重参数,使得我们能够在需要时重新加载并使用这个训练好的神经网络模型。"
    },
    {
        "index": 48,
        "type": "SingleChoice",
        "question": "以下哪个部件是神经⽹络的核⼼元素?( )",
        "options": {
            "A": "权重和激活函数",
            "B": "全连接层和softmax",
            "C": "神经元和权重",
            "D": "张量和ReLU"
        },
        "answers": [
            "C"
        ],
        "explanation": "在分析神经网络的核心元素时,我们需要考虑构成神经网络的基本组件和它们如何共同作用以学习并处理数据。选项A中的两个元素在神经网络中非常重要,但它们并不是神经网络的完整核心。权重用于调节神经元的输入和输出之间的连接强度,而激活函数决定了神经元是否应该被激活(即是否应该传递信号)。然而,它们只是神经元的一部分。在选项B中,全连接层是神经网络的一种层类型,其中每个神经元都连接到前一层和后一层的所有神经元。softmax通常用于多分类问题的输出层,将神经网络的输出转换为概率分布。然而,全连接层和softmax并不是神经网络的核心元素,而是构建神经网络的特定层类型。选项C正确,神经元是神经网络的基本单元,它们接收输入,对这些输入进行加权求和,并应用激活函数以产生输出。权重则是连接神经元之间的参数,它们决定了每个输入对神经元输出的贡献程度。神经元和权重共同构成了神经网络的基本结构和功能,是神经网络的核心元素。选项D错误,张量是神经网络中用于表示多维数组的数据结构,而ReLU是一种常用的激活函数。虽然张量和ReLU在神经网络中都有重要作用,但它们并不是神经网络的核心元素。"
    },
    {
        "index": 49,
        "type": "SingleChoice",
        "question": "在深度学习中,⽤于调节权重的核⼼算法是什么?()",
        "options": {
            "A": "梯度下降",
            "B": "反向传播",
            "C": "前向传播",
            "D": "线性回归"
        },
        "answers": [
            "B"
        ],
        "explanation": "在深度学习中,用于调节权重的核心算法是反向传播(Backpropagation)。反向传播算法是深度学习的基石,它通过调整网络权重以最小化损失函数(或目标函数)来学习和优化神经网络。虽然梯度下降是优化神经网络参数的一种常用方法,但它本身并不直接涉及权重的调节。梯度下降是一个更广泛的概念,用于寻找函数的最小值,而反向传播用于将计算出的梯度来更新权重。前向传播是神经网络中数据从输入层流向输出层的过程,用于计算网络的输出值。线性回归是一种简单的统计方法,用于建立因变量与自变量之间的线性关系,它并不涉及深度学习中权重的调节或反向传播算法。综上所述,反向传播算法是深度学习中用于调节权重的核心算法,它通过计算损失函数的梯度并逐层反向传播误差来更新网络的权重和偏置,从而实现神经网络的优化。因此,正确答案是B。"
    },
    {
        "index": 50,
        "type": "SingleChoice",
        "question": "机器学习学习过程的本质是( )。",
        "options": {
            "A": "找到⼀组模型参数,使得损失函数最⼤化",
            "B": "找到⼀组模型参数,使得损失函数最⼩化",
            "C": "找到⼀组模型参数,使得损失函数等于0",
            "D": "找到⼀组模型参数,使得损失函数等于1"
        },
        "answers": [
            "B"
        ],
        "explanation": "在机器学习的学习过程中,我们的目标是找到一个模型(或一组模型参数),这个模型能够最好地拟合训练数据,并且在新数据上也能有良好的表现。为了量化模型的表现,我们通常定义一个损失函数(loss function),它度量了模型的预测值和真实值之间的差距。选项A是错误的,因为我们的目标是找到使损失函数值尽可能小的模型参数,而不是最大化损失函数。选项C在实际中通常是不现实的。除非数据是线性可分的,且模型有足够的复杂性(例如,对于线性可分的数据使用线性模型),否则很难找到一个模型使得损失函数在所有数据点上都等于0。此外,即使存在这样的模型,它也可能出现过拟合问题。选项D同样是错误的,损失函数的值取决于数据和模型,而不是一个固定的数(如1)。我们的目标是找到使损失函数值尽可能小的模型参数,而不是使损失函数等于某个特定的数。"
    },
    {
        "index": 51,
        "type": "SingleChoice",
        "question": "以下描述神经⽹络的⼯作原理正确的是( )。",
        "options": {
            "A": "通过观察⽰例学习数据变换",
            "B": "将输⼊映射到⽬标的过程",
            "C": "通过⼀系列简单的数据变换实现输⼊到⽬标的映射",
            "D": "通过观察⽰例学习权重"
        },
        "answers": [
            "C"
        ],
        "explanation": "神经网络的核心功能是将输入数据映射到期望的输出或目标。这一映射是通过神经网络内部的多层结构和神经元之间的连接实现的。在神经网络中,数据变换是通过神经元之间的连接权重和激活函数来实现的。权重是通过学习得到的,即神经网络通过观察示例(训练数据)来调整权重,以优化映射的准确性和效率。"
    },
    {
        "index": 52,
        "type": "SingleChoice",
        "question": "深度学习中,什么函数⽤于衡量预测值与期望值之间的差距?( )",
        "options": {
            "A": "激活函数",
            "B": "损失函数",
            "C": "优化器",
            "D": "梯度下降"
        },
        "answers": [
            "B"
        ],
        "explanation": null
    },
    {
        "index": 53,
        "type": "SingleChoice",
        "question": "以下编程语⾔最适合⼈⼯智能,特别是深度学习领域的是( )。",
        "options": {
            "A": "Java",
            "B": "PHP",
            "C": "Python",
            "D": "Html"
        },
        "answers": [
            "C"
        ],
        "explanation": "在早期,从事深度学习需要精通C++和CUDA,而它们只有少数人能掌握;今天,具有基本的Python脚本技能,就可以从事高级的深度学习研究,得益于Caffe,Theano,MXNet及随后的TensorFlow,Pytorch、Keras、PaddlePadle等用户友好型框架的兴起,深度学习应用的开发就像操作乐高积木一样简单。当然,C++和CUDA依然是深度学习底层开放人员所必备的知识。"
    },
    {
        "index": 54,
        "type": "SingleChoice",
        "question": "深度学习与传统机器学习相⽐,最⼤的不同是摒弃了特征⼯程的繁琐,使得模型能够完全⾃动化地从原始数据获取信息,这种⽅法称为()。",
        "options": {
            "A": "去特征化学习",
            "B": "端到端学习",
            "C": "⽆监督学习",
            "D": "⾃动特征抽取学习"
        },
        "answers": [
            "B"
        ],
        "explanation": "选项A,去特征化学习的描述不准确,在深度学习中并没有“去特征化学习”这一说法,而是自动化地学习并抽取数据的特征。选项B正确,端到端学习指的是模型能够直接从原始输入数据学习并输出最终结果,无需人工干预或进行特征工程的步骤。它允许模型自动发现数据的表示和特征,从而简化了机器学习模型的构建过程。选项C,无监督学习是机器学习的一种类型,但它并不特指深度学习与传统机器学习在特征工程方面的差异。无监督学习主要处理没有标签的数据,目的是发现数据中的隐藏结构或模式。选项D,特征自动抽取不正确。虽然这个选项描述了深度学习的一个关键特性,即自动抽取特征,但它没有涵盖从原始数据到最终输出的完整过程,因此不如“端到端学习”全面。"
    },
    {
        "index": 55,
        "type": "Judgement",
        "question": "Anaconda是⼀个功能丰富的Python集成开发环境,特别适⽤于软件包的管理和安装。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": null
    },
    {
        "index": 56,
        "type": "Judgement",
        "question": "Python在深度学习中被⼴泛使⽤,主要因为它的语法简单易懂和拥有丰富的库和框架。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": "Python因其简单易用、开放源代码、可扩展的特点,在深度学习中被广泛使用,并拥有丰富的库和框架,如TensorFlow、Keras、PyTorch等。\n"
    },
    {
        "index": 57,
        "type": "Judgement",
        "question": "神经⽹络的训练总是需要⼤量的计算资源。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": "神经网络的训练通常涉及大量的矩阵运算和参数更新,因此需要大量的计算资源。然而,随着硬件技术的发展和优化算法的改进,训练神经网络所需的资源正在逐渐减少。\n"
    },
    {
        "index": 58,
        "type": "Judgement",
        "question": "在神经⽹络中,每个神经元都接收来⾃前⼀层所有神经元的输出。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "在神经网络中,全连接层(dense layer)的每个神经元通常接收来自前一层所有神经元的输出,但在卷积神经网络(CNN)等结构中,神经元可能只接收前一层的部分输出(即局部感受野)。\n\n \n  \n\n"
    },
    {
        "index": 59,
        "type": "Judgement",
        "question": "神经⽹络的层数越多,其性能⼀定越好。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "神经网络的性能并不总是随着层数的增加而提高。过深的网络可能导致过拟合、梯度消失或梯度爆炸等问题,反而降低模型的性能。\n"
    },
    {
        "index": 60,
        "type": "Judgement",
        "question": "神经⽹络的训练过程总是能找到全局最优解。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "神经网络的训练过程通常只能找到局部最优解,而非全局最优解。全局最优解的寻找在复杂的神经网络中是一个NP难问题。\n"
    },
    {
        "index": 61,
        "type": "Judgement",
        "question": "局部最⼤值不⼀定是全局最⼤值,但极⼤值通常都是全局最⼤值。(  )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "极值是用来形容一定的领域内的最值,但并一定是全域的最值。因此,极值一般表示的都是局部最优值,并不一定是全局最优值。\n"
    },
    {
        "index": 62,
        "type": "Judgement",
        "question": "解析法⼜称分析法是使⽤解析式求解数学模型的常⽤⽅法。这种⽅法对于求解神经⽹络模型同样适⽤。( )",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "给定一个可微函数,理论上可以使用解析法求其最小值,即导数为0的点所对应的函数位置的值。对于一个神经元来说,理论上,可以通过其梯度方程 gradient(f)(W)=0 求解权重W。然而,事实上这并不现实。对于任何一个神经网络来说,权重参数的个数至少都有数千,甚至上百万,解析法无法进行求解。\n"
    },
    {
        "index": 63,
        "type": "MultipleChoice",
        "question": "以下哪些集成开发环境属于常⽤深度学习应⽤开发⼯具的是( )。",
        "options": {
            "A": "Visual Studio Code",
            "B": "JupyterLab",
            "C": "Photoshop",
            "D": "PyCharm"
        },
        "answers": [
            "A",
            "B",
            "D"
        ],
        "explanation": "一般来说,使用什么开发环境对于程序设计语言来说并没有绝对的对应关系,甚至于使用文本文档也可以进行程序的开发。不过,一些开发工具会被广泛使用,也证明其对某个领域的易用性和适应性,例如VSCode, PyCharm就是Python程序开发最常用的IDE,此外Jupyter Notebook(JupyterLab)作为数据分析领域最优秀的开发环境,也常常被用来进行人工智能应用的开发和实践。 "
    },
    {
        "index": 64,
        "type": "MultipleChoice",
        "question": "以下⼯具包可以⽤于深度学习应⽤的是( )。",
        "options": {
            "A": "Caffe及Caffe2",
            "B": "scikit-learn",
            "C": "PaddlePaddle",
            "D": "TensorFlow",
            "E": "Pytorch",
            "F": "OpenCV"
        },
        "answers": [
            "A",
            "C",
            "D",
            "E"
        ],
        "explanation": "深度学习如今已经成为机器学习最炙手可热的研究方法,很多工具包被开发出来,CNTK,Caffe,pytorch,Keras,theano,Tensorflow,paddlepaddle,mxnet都是其中的佼佼者。著名的机器学习工具包scikit-learn几乎支持所有的传统机器学习算法,但对于深度学习略显无力,它只支持简单的MLP模型,但其数据预处理功能经常被应用到其他工具包。OpenCV是一个开源的计算机视觉库,它提供了很多函数,这些函数非常高效地实现了计算机视觉算法(最基本的滤波到高级的物体检测皆有涵盖)"
    },
    {
        "index": 65,
        "type": "MultipleChoice",
        "question": "除了硬件和数据之外,⼀些可靠的⽅法⽤于实现较深的神经⽹络,它们包括:( )。",
        "options": {
            "A": "ReLU激活函数",
            "B": "Xavier权重初始化⽅案",
            "C": "随机梯度下降SGD",
            "D": "批标准化BatchNorm"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "从2009年开始,一些算法的产生带来了深度学习的发展,这主要包括激活函数、权重初始化方案和优化方法。"
    },
    {
        "index": 66,
        "type": "MultipleChoice",
        "question": "深度学习在2012年后取得了成功,这主要是因为三种技术推动了机器学习的进步,它们包括:( )。",
        "options": {
            "A": "硬件,尤其是⾼性能的图形芯⽚GPU的出现",
            "B": "数据集和基准",
            "C": "先进的数学理论的产⽣",
            "D": "算法上的改进"
        },
        "answers": [
            "A",
            "B",
            "D"
        ],
        "explanation": "深度学习用于计算机视觉的两个关键思想,即卷积神经网络和反向传播,这个思想在1989年就为人所知(LeNet)。长短期记忆(LSTM)算法是深度学习处理时间序列的基础,在1997年被开发出来。今天,LSTM依然是自然语言处理最有效的方法。"
    },
    {
        "index": 67,
        "type": "MultipleChoice",
        "question": "深度学习能在计算机视觉和语⾳识别等感知任务上具有⾮凡的效果,最重要的原因是来源于两个思想( )。",
        "options": {
            "A": "基于概率的数学推导",
            "B": "卷积神经⽹络",
            "C": "决策边界理论",
            "D": "反向传播"
        },
        "answers": [
            "B",
            "D"
        ],
        "explanation": "深度学习用于计算机视觉的两个关键思想,即卷积神经网络和反向传播,这个思想在1989年就为人所知(LeNet)。长短期记忆(LSTM)算法是深度学习处理时间序列的基础,在1997年被开发出来。今天,LSTM依然是自然语言处理最有效的方法。"
    },
    {
        "index": 68,
        "type": "Judgement",
        "question": "在原始的Alexnet中,作者使⽤2块GTX 580 GPU来进⾏训练,在这两个GPU上都有完整的Alexnet模型。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "在Alexnet中,作者所使用的GPU并行计算,通常称为模型并行。使用该方法的原因是,Alexnet需要用到超过3GB的现存,而当时最好的显卡GTX 580 只有3GB。因此,Alex就将模型拆分成两组,除了输入和最终的输出部分,模型都被平均分成两份,放在两个不同的GPU显卡上。因此,在两个GPU上实际上是没有完整模型的。\n\n \n  \n\n"
    },
    {
        "index": 69,
        "type": "Judgement",
        "question": "多模型融合⼀般指使⽤不同类型的模型结构进⾏融合,同时使⽤多个CNN进⾏融合,⼀般⽆法起到提⾼性能的作⽤。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "无论是使用多个不同类型的模型,还是使用同类型,甚至同结构的模型进行融合都会有一定的效果。特别是对于CNN来说,由于每个模型都是使用随机初始化,同时还使用Dropout,因此每次训练获得的模型,都有其独特性(显著性),通过融合不同次训练的模型,可以让这种独特性往好的方向发展,从而提高系统性能。\n"
    },
    {
        "index": 70,
        "type": "MultipleChoice",
        "question": "下列多级学习率设计合理的包括()。",
        "options": {
            "A": "[0.01, 0.001, 0.0001]",
            "B": "[0.0001, 0.001, 0.01]",
            "C": "[0.0001, 0.01, 0.001, 0.0001]",
            "D": "[0.001, 0.01, 0.001, 0.01]"
        },
        "answers": [
            "A",
            "C"
        ],
        "explanation": "A选项,为常规多级学习率的设置方法,一开始使用较大的学习率使其能够快速收敛,后面逐渐减小以实现微调;C选项,为增加了慢启动的多级学习率,一开始较小的学习率能够实现一定程度的适应,比直接从高斯随机初始化进行训练能够获得更好是起点;B选项,学习率递增没有意义;D选项,抖动的学习率也没有意义。 "
    },
    {
        "index": 71,
        "type": "MultipleChoice",
        "question": "下列哪些项,包含在Alexnet的权重更新规则中?()",
        "options": {
            "A": "动量项",
            "B": "权重衰减项",
            "C": "梯度提升项",
            "D": "梯度项"
        },
        "answers": [
            "A",
            "B",
            "D"
        ],
        "explanation": "权重(梯度)更新规则为:<span class=\"math inline\">\n    $v_{i + 1}:\n= 0.9v_{i} - 0.0005\\epsilon w_{i} - \\epsilon &lt; \\frac{\\partial\nL}{\\partial w}|w_{i} &gt; D_{i}$\n   </span> ;<span class=\"math inline\">\n    <em>\n     w\n    </em>\n    <sub>\n     <em>\n      i\n     </em>\n     + 1\n    </sub>\n    :  =\n    <em>\n     v\n    </em>\n    <sub>\n     <em>\n      i\n     </em>\n     + 1\n    </sub>\n   </span> "
    },
    {
        "index": 72,
        "type": "MultipleChoice",
        "question": "Alexnet模型的原⽂中,实现了以下哪些任务?()",
        "options": {
            "A": "图像分类",
            "B": "⽬标检测和识别",
            "C": "图像检索",
            "D": "图像分割"
        },
        "answers": [
            "A",
            "B",
            "C"
        ],
        "explanation": "分类、检测、检索、分割是计算机视觉的四大任务,在Alexnet的原始论文《ImageNet Classification with Deep Convolutional Neural Networks》,作者主要实现了图像分类、目标识别和定位、图像检索三大任务。"
    },
    {
        "index": 73,
        "type": "MultipleChoice",
        "question": "在使⽤卷积神经⽹络进⾏图像查询的时候,⼀般使⽤()来进⾏相似性对⽐。",
        "options": {
            "A": "交叉熵",
            "B": "欧氏距离",
            "C": "汉明距离",
            "D": "贝叶斯概率"
        },
        "answers": [
            "B",
            "C"
        ],
        "explanation": "A选项常被应用在分类任务中;B、C选项用于计算两个样本的相似性(或距离),可以用于图像检索,也可以用于目标检测的回归分支;D选项用于评价信息的置信度"
    },
    {
        "index": 74,
        "type": "SingleChoice",
        "question": "ZFNet对AlexNet进⾏了()的优化,并最终获得了ILSVRC2014的冠军,ZFNet也证明了合理的超参数设置对于⽹络的性能具有很⼤的影响。",
        "options": {
            "A": "超参数",
            "B": "模型宽度",
            "C": "模型深度",
            "D": "输⼊图像尺度"
        },
        "answers": [
            "A"
        ],
        "explanation": " ZFNet对AlexNet进行了超参数的优化,并最终获得了ILSVRC2014的冠军。ZFNet的具体优化包括:CONV1: 卷积核 (11x11 stride 4) 改为 (7x7 stride 2);CONV3,4,5: 卷积核数量 384, 384, 256 改为 512, 1024, 512;对比正则优化 contrast norm "
    },
    {
        "index": 75,
        "type": "SingleChoice",
        "question": "下列关于模型结束训练描述正确的是()。",
        "options": {
            "A": "当错误率⼩于0.01时可以停⽌训练",
            "B": "需要等待所有轮次训练完才能结束训练",
            "C": "当训练损失仍在下降时不应该结束训练",
            "D": "当验证集损失停⽌下降时应该结束训练"
        },
        "answers": [
            "D"
        ],
        "explanation": "在进行模型训练时,最佳的结束训练时间是验证集损失停止下降时。通常,当验证集损失不再下降就应该及时停止训练,避免过拟合的产生,这种策略也成为早期停止(early-stop). C选项,训练损失仍在下降,而验证损失不在下降就是典型的过拟合现象。"
    },
    {
        "index": 76,
        "type": "SingleChoice",
        "question": "在AlexNet的训练过程中,基于动量的随机梯度下降被⼴泛使⽤,并且使⽤了步进学习率。即当验证误差趋于平稳,不再下降时,学习率()。",
        "options": {
            "A": "提⾼100倍",
            "B": "提⾼10倍",
            "C": "保持不变",
            "D": "降低10倍",
            "E": "降低100倍"
        },
        "answers": [
            "D"
        ],
        "explanation": "步进学习率时训练深度神经网络时常用的学习率策略,它指在学习过程中,当验证误差不再下降时(趋于平稳态),学习率降低10倍。"
    },
    {
        "index": 77,
        "type": "SingleChoice",
        "question": "在卷积神经⽹络中,基于动量的随机梯度下降被⼴泛使⽤,其中超参数Momentum的典型值为()。",
        "options": {
            "A": "0",
            "B": "0.5",
            "C": "0.9",
            "D": "1.0"
        },
        "answers": [
            "C"
        ],
        "explanation": "在权重更新公式中,动量Momentum=0.9, 权重衰减系数=0.0005,为一组典型值。 "
    },
    {
        "index": 78,
        "type": "SingleChoice",
        "question": "在卷积神经⽹络中,输出层输出的预测标签为输出概率的对数中()的索引。",
        "options": {
            "A": "平均值",
            "B": "最⼩值",
            "C": "最⼤值",
            "D": "⽆法判定"
        },
        "answers": [
            "C"
        ],
        "explanation": "最大值"
    },
    {
        "index": 79,
        "type": "SingleChoice",
        "question": "在深度学习的训练过程中,有时候会⽤到多块GPU进⾏多GPU并⾏训练,这种⽅法最⼤的优势是()。",
        "options": {
            "A": "缩短训练时间",
            "B": "提⾼训练精度",
            "C": "减少硬盘空间的占⽤",
            "D": "降低模型对总数据量的需求"
        },
        "answers": [
            "A"
        ],
        "explanation": "深度学习常用的多GPU并行训练,通常指基于Batch划分的多GPU并行。在训练过程中,可以将不同Batch的数据分配到不同的GPU上进行同时训练,相当于原来在一个GPU上训练的工作,被分配到多个GPU上共同承担,这样可以大大缩短训练时间。"
    },
    {
        "index": 80,
        "type": "SingleChoice",
        "question": "GPU并⾏计算是深度学习模型能够顺利训练的关键因素,⽬前在模型训练中,最常⽤的多GPU并⾏⽅法是()。",
        "options": {
            "A": "模型并⾏",
            "B": "参数并⾏",
            "C": "数据并⾏",
            "D": "混合并⾏"
        },
        "answers": [
            "C"
        ],
        "explanation": "模型并行:将模型拆分成不同的部分,并在不同的GPU上运行。数据并行:每个GPU上都存有模型的副本,将不同的数据同时分配到各个GPU上进行训练。参数并行:并行系统中,通常有一个参数服务器,由其负责管理每个服务器的参数分配。深度学习的GPU并行,通常指基于Batch划分的多GPU并行。不同的数据batch会被分到不同的GPU上进行训练,并在最后进行梯度合并,然后再划分到不同的GPU上执行反向传播更新梯度。"
    },
    {
        "index": 81,
        "type": "SingleChoice",
        "question": "Softmax概率归⼀化函数的主要功能是实现()。",
        "options": {
            "A": "将神经⽹络获得的特征值(分值)转换为概率分布",
            "B": "计算⽹络输出的归⼀化概率和真是标签之间的交叉熵",
            "C": "获取神经⽹络输出对应的One-hot向量",
            "D": "获取⽹络的输出分值"
        },
        "answers": [
            "A"
        ],
        "explanation": "神经网络的多分类损失主要包含两部,第一步是使用Softmax概率归一化函数实现将神经网络的输出分值归一化为概率分布;第二步是计算归一化概率和真实标签之间的交叉熵。交叉熵可以用来衡量两个概率之间的距离。"
    },
    {
        "index": 82,
        "type": "MultipleChoice",
        "question": "下列技术可以对输出激活实现降采样的有哪些?()",
        "options": {
            "A": "Convolution",
            "B": "Padding",
            "C": "Max-Pooling",
            "D": "Stride",
            "E": "Mean-Pooling"
        },
        "answers": [
            "B",
            "C",
            "D"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>池化技术和步长Stride技术都可以实现对输出激活(特征图)的降采样。</p></div>"
    },
    {
        "index": 83,
        "type": "SingleChoice",
        "question": "下列选项中,被Alex认为可以通过局部响应归⼀化实现值域限制来改进性能的激活函数包括()。",
        "options": {
            "A": "ReLU",
            "B": "Sigmoid",
            "C": "Tanh",
            "D": "Softmax"
        },
        "answers": [
            "A"
        ],
        "explanation": "Sigmod和Tanh有一定的归一化作用,它们将输出归一化到[0,1]/[-1,1]之间。而ReLU的值域并没有限制,因此需要对其进行归一化。Alexnet提出了一种归一化方法,称为局部响应归一化 (Local Response Normalization,LRN)。基本想法是通过LRN层,对局部神经元的活动创建竞争机制,使得其中响应比较大的值变得相对更大,并抑制其他反馈较小的神经元,增强模型的泛化能力。 "
    },
    {
        "index": 84,
        "type": "SingleChoice",
        "question": "以下步⻓和池化核尺度的关系中,属于有重叠池化的⼀个是。()",
        "options": {
            "A": "pool_size = 2, stride = 2",
            "B": "pool_size = 2, stride = 3",
            "C": "pool_size = 3, stride = 2",
            "D": "pool_size = 3, stride = 3"
        },
        "answers": [
            "C"
        ],
        "explanation": "有重叠池化是指步长小于池化核时,相邻的池化运算具有重叠的区域。一般来说,pool_size = stride称为无重叠池化;stride<pool_size,则称为有重叠池化."
    },
    {
        "index": 85,
        "type": "SingleChoice",
        "question": "下列哪⼀项时Dropout的典型值?()",
        "options": {
            "A": "0",
            "B": "0.5",
            "C": "0.9",
            "D": "1"
        },
        "answers": [
            "B"
        ],
        "explanation": null
    },
    {
        "index": 86,
        "type": "SingleChoice",
        "question": "下列为了实现保留输⼊特征图的维度,从⽽使输出特征图具有和输⼊特征图⼀样的维度的技术是哪⼀个?()",
        "options": {
            "A": "Convolution",
            "B": "Padding",
            "C": "Max-Pooling",
            "D": "Stride",
            "E": "Mean-Pooling"
        },
        "answers": [
            "B"
        ],
        "explanation": "Padding技术实现在输入特征图周围进行补0操作,从而实现在输出激活中保留输入空间的维度,即使输出特征图具有和输入特征图一样的维度。"
    },
    {
        "index": 87,
        "type": "SingleChoice",
        "question": "以下常⻅的激活函数,哪⼀个收敛速度最快?()",
        "options": {
            "A": "Sigmoid",
            "B": "SoftPlus",
            "C": "Tanh双曲正切函数",
            "D": "ReLU限制线性单元"
        },
        "answers": [
            "D"
        ],
        "explanation": "在Alex给出的实验中,证明ReLU由于其线性特性,因此它的导数始终为1,所以大大减少了反向传播时的计算量,这给优势让它比Sigmoid和Tanh快了近6倍。相似的,SoftPlus也是非线性的激活函数,因此执行效率和Sigmoid近似。"
    },
    {
        "index": 88,
        "type": "Judgement",
        "question": "灰度图和彩⾊图不同,通常不但需要做均值消除,还需要做标准化处理。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "灰度图通常只需要将色彩的数值模式从0-255转换为0-1之间,而不需要再做减均值操作,也不现需要做标准化处理。但是,在一些任务中,需要执行二值化操作,即使用阈值的方法,将所有像素的值都设置成0或1。\n\n \n  \n\n"
    },
    {
        "index": 89,
        "type": "MultipleChoice",
        "question": "数据增⼴是基于深度学习任务中⾮常重要的⼀个环节,可以有效地解决过拟合问题,使⽤数据增⼴的主要原因包括( )。",
        "options": {
            "A": "深度模型的参数太多",
            "B": "相对于深度模型来说,数据太少",
            "C": "深度模型的神经元太多",
            "D": "很多时候数据的多样性不⾜"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "数据增广(Data Augmentation)又称为数据扩充,通常包括平移、旋转、色彩变换、缩放、遮挡、裁剪、水印、光照等多种变换。对原始数据集进行数据增广,既可以增加数据量,又可以增加样本的多样性,从而消除因为样本数量少或者样本中对象特殊性过引起的过拟合问题。同时,由于深度神经网络通常具有较多的参数和神经元,这非常容易产生过拟合问题,大规模数据的引入可以在很大程度上缓解这种过拟合问题。 "
    },
    {
        "index": 90,
        "type": "MultipleChoice",
        "question": "在计算机视觉的任务中,以下属于均值消除带来的优点的包括()。",
        "options": {
            "A": "提⾼神经⽹络的收敛速度,从⽽降低训练时间",
            "B": "降低像素间的相关性,减少模型复杂度,从⽽实现模型瘦⾝",
            "C": "降低像素间的相关性,从⽽提⾼特征的显著性,进⽽提⾼模型的拟合能⼒",
            "D": "减少噪声数据对模型的影响,从⽽提⾼新模型的性能"
        },
        "answers": [
            "A",
            "C"
        ],
        "explanation": "零均值有助于避免Z型更新的情况,一方面提高神经网络收敛速度,另外一方面也能避免像素间因为均值的相互影响导致特征显著性的下降。"
    },
    {
        "index": 91,
        "type": "SingleChoice",
        "question": "在Alexnet模型的测试过程中,原始的测试数据被扩展了()倍。",
        "options": {
            "A": "2",
            "B": "10",
            "C": "1024",
            "D": "2048"
        },
        "answers": [
            "B"
        ],
        "explanation": "在测试阶段,通常使用10重切割进行数据扩展。即在256×256的样本中执行左上,左下,右上,右下,中间5次裁剪,并对裁剪数据执行水平翻转。预测时,对10个样本的预测概率求平均值。 "
    },
    {
        "index": 92,
        "type": "SingleChoice",
        "question": "在Alexnet模型中,不计算⾊彩饱和度变换,原始的训练数据被扩展了()倍。",
        "options": {
            "A": "2",
            "B": "10",
            "C": "1024",
            "D": "2048"
        },
        "answers": [
            "D"
        ],
        "explanation": "训练阶段,执行随机裁剪+水平反转:对256×256的图片进行顺序裁剪至224×224,并对每个切片都执行水平反转。数据增加量=(256−224)2×2=2048=(256−224)2×2=2048倍"
    },
    {
        "index": 93,
        "type": "SingleChoice",
        "question": "观察如下数据预处理的数学表达式,请问哪⼀项是Alexnet所使⽤的均值消除⽅式?()",
        "options": {
            "A": "$x^{\\ast} = \\frac{x - x_{\\min}}{x_{\\max} -\nx_{\\min}}$\n​​",
            "B": "$x^{\\ast} = \\frac{x -\n\\mu}{\\sigma}$\n​",
            "C": "x\n*\n=\nx\n−\nμ"
        },
        "answers": [
            "C"
        ],
        "explanation": "标准化和归一化都具有消除量纲的功能,使得原本分布相差较大的特征调整为对模型具有相同权重的影响,在传统及其学习中使用非常普遍。如果期望在最后的计算中,让所有特征都具有相同作业,则使用标准化;若希望保留原始数据中标准差所反映的潜在权重关系,则使用归一化。此外,标准化更加适合具有较多噪声的大数据场景。考虑到对于图像或视频或音频的数据的预处理,由于使用的都是同样类型的数据(图像视频为像素、音频为波形),所有的样本点都是一种特征,例如图像都分布在 [0,255]或[0,1] 之间。因此,不存在量纲问题。所以,只需要做零均值化即可。当然,选项B,方差归一化也是深度学习中常用的预处理方法。"
    },
    {
        "index": 94,
        "type": "SingleChoice",
        "question": "在Alexnet模型中,⼀般使⽤()⽅式进⾏均值减除。",
        "options": {
            "A": "逐像素求均值",
            "B": "按⾊彩通道求均值",
            "C": "按⾏求均值",
            "D": "按列求均值"
        },
        "answers": [
            "B"
        ],
        "explanation": "在进行图像预处理时,求均值是一个非常重要的操作,它可以消除像素间的相关性。常见的求均值包括:逐像素、逐行、逐列、按色彩通道。在基于深度学习的模型中,按色彩通道求平均时效果最好的。在Alexnet中,作者使用128万的训练级完成了三个色彩通道的均值计算,该值为[104,117,123]。一般认为这个均值是比较符合自然场景的均值,因此被广泛应用到大多数通用的计算机视觉任务中。"
    },
    {
        "index": 95,
        "type": "Judgement",
        "question": "在卷积神经⽹络中,全连接层的参数个数的⽐例通常较⾼,因此全连接层也是特征学习的主要来源。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "相比全连接层,卷积层能够更好地还原样本的特征,因此卷积层才是特征学习的主要来源。不过,全连接层特征通常会被作为样本最终的表达送入下游任务中。\n\n \n  \n\n"
    },
    {
        "index": 96,
        "type": "MultipleChoice",
        "question": "在7层模型Alexnet⽹络中,哪些层包含有最⼤池化层(Max-Pooling)?()",
        "options": {
            "A": "卷积层Conv1",
            "B": "卷积层Conv2",
            "C": "卷积层Conv3",
            "D": "卷积层Conv4",
            "E": "卷积层Conv5",
            "F": "全连接层FC6",
            "G": "全连接层FC7"
        },
        "answers": [
            "A",
            "B",
            "E"
        ],
        "explanation": "池化层一般指出现卷积层后,但是在Alexnet中,只有第一、二、五个卷积层(组)后面紧跟了一个最大池化层。 "
    },
    {
        "index": 97,
        "type": "MultipleChoice",
        "question": "AlexNet最⼤的贡献是⽅法论的进化,以下描述出正确的包括()。",
        "options": {
            "A": "特征提取从纯⼿⼯向模型⾃动提取转变",
            "B": "样本的特征选择逐渐由程序员⾃主选择转变为由领域专家进⾏设计和选择",
            "C": "使⽤端到端的设计思路,将特征提取模型及分类器模型合并成⼀个模型进⾏同时训练",
            "D": "按照分布设计的思想,将特征提取模型及分类器模型分开进⾏训练,并将分别训练好的最后模型进⾏组合输出"
        },
        "answers": [
            "A",
            "C"
        ],
        "explanation": "在机器学习时代,对于一张图片,首先要进行人工的特征提取。计算机视觉的研究者主要关注的就是如何实现更好的特征提取。所以,专家对整个问题的理解主要是放在手工特征提取这一块,他们的主要工作就是将对问题的理解转换为标准的机器学习算法所理解的数值。另一方面,对于深度卷积神经网络来说。它们的最后一层就是一个Softmax回归,之前的所有层可以看成是一个通过CNN来学习特征的特征学习和提取器。CNN的主要好处是,分类器之前的部分不再像传统机器学习模型一样是独立的了,特征提取与分类器成为了一个统一的整体,一起进行训练。这就意味着,CNN学出来的东西很有可能就是你Softmax想要的。我通过我的很深的神经网络把整个原始的像素能够映射到一个空间,使得你的Softmax能够很好地进行分类。对于AlexNet模型来说,有两点是值得肯定的。第一是构造CNN相对来说比较简单,不需要了解很多专业的计算视觉的知识,而且能够很好地跨越到其他不同的学科。第二点是说特征提取和分类器其实是在一起在训练的,从模型的角度来看它们其实就是一个模型,这样的模型也比分离式的机器学习模型更加高效。这也是深度学习相对于传统机器学习来说最大的改变。换句话说,我们不需要再去挣扎如何对样本进行特征的抽取,而是一种端到端的学习。模型可以从原始的像素、字符串开始,直接将信号传送给输出,并直接生成分类或预测。可以说,端到端学习是深度学习最大的一个卖点。"
    },
    {
        "index": 98,
        "type": "SingleChoice",
        "question": "模型的复杂度是⽤来衡量模型所占⽤资源的依据,就LeNet和AlexNet来说,下列对模型复杂度描述正确的是()。",
        "options": {
            "A": "[55×55×96]",
            "B": "[110,110,96]",
            "C": "[111,111,96]",
            "D": "[111,111,3]"
        },
        "answers": [
            "C"
        ],
        "explanation": "按照计算公式W2=(W1-F+2P)/S+1,可以得到输出特征图的平面维度=(227-7+0)/2+1=111,深度维度=卷积核的个数=96。因此,最终的输出尺度为:[111,111,96] "
    },
    {
        "index": 99,
        "type": "SingleChoice",
        "question": "在Alexnet模型中,输⼊样本的尺度为[227×227×3],若卷积核的尺度变为[7×7×3],卷积核的步⻓为2,padding为0,个数为96,则输出特征图的尺度为:()。",
        "options": {
            "A": "[55×55×96]",
            "B": "[110,110,96]",
            "C": "[111,111,96]",
            "D": "[111,111,3]"
        },
        "answers": [
            "C"
        ],
        "explanation": "按照计算公式W2=(W1-F+2P)/S+1,可以得到输出特征图的平面维度=(227-7+0)/2+1=111,深度维度=卷积核的个数=96。因此,最终的输出尺度为:[111,111,96]"
    },
    {
        "index": 100,
        "type": "SingleChoice",
        "question": "在Alexnet模型中,参数最多的⼀类层是(),最少的⼀类层是()。",
        "options": {
            "A": "卷积层 池化层",
            "B": "全连接层\n池化层",
            "C": "全连接层 卷积层",
            "D": "池化层 卷积层"
        },
        "answers": [
            "B"
        ],
        "explanation": "在所有的卷积神经网络中,参数最多的是全连接层,该层参数的个数由前后两层的神经元个数决定;参数最少池化层,该层的参数个数为0。"
    },
    {
        "index": 101,
        "type": "SingleChoice",
        "question": "在Alexnet⽹络模型中,全连接层(不含FC8,且只计算单层)的神经元数量为()个。",
        "options": {
            "A": "256",
            "B": "384",
            "C": "1000",
            "D": "2048",
            "E": "4096"
        },
        "answers": [
            "E"
        ],
        "explanation": "在标准的Alexnet中,包含两个神经元个数为4096的全连接层,但是根据目标任务的难易程度,可以对全连接层的神经元个数进行调整,通常对于类别较少的任务,神经元的个数也会做相应的减少。"
    },
    {
        "index": 102,
        "type": "SingleChoice",
        "question": "从学习⽅法来看,Alexnet模型属于()。",
        "options": {
            "A": "监督学习",
            "B": "⾮监督学习",
            "C": "半监督学习",
            "D": "弱监督学习"
        },
        "answers": [
            "A"
        ],
        "explanation": "Alexnet是最早被应用在大规模图像分类上卷积神经网络模型,该模型使用Softmax和交叉熵来实现分类,在计算距离的时候,需要原始数据的标签来计算欧氏距离,因此属于监督模型。"
    },
    {
        "index": 103,
        "type": "SingleChoice",
        "question": "为了能实现更好的特征表达能⼒,除了卷积和池化,卷积神经⽹络还需要()。",
        "options": {
            "A": "更宽的⽹络⽀持",
            "B": "更深的⽹络",
            "C": "更多的神经元",
            "D": "更⼤的卷积核"
        },
        "answers": [
            "B"
        ],
        "explanation": "更深的网络已被证明是提高复杂样本识别率最有效的方法之一,深度学习的名字即来源于其更深的深度。更宽的网络在某些情况下也可以提高系统的识别力,但该方法并不总是有效。更多的神经元意味着深度和宽度的增加,这一点可以参考深度和宽度的解释。更大的卷积核对于卷积神经网络来说可以学到更广的局部特性,但同样也会损害小样本局部特性的学习,同时也会大幅度地增加权重参数的数量。事实上,大多数CNN都使用尺度较小的卷积核,典型的尺度为3×33×3和1×11×1."
    },
    {
        "index": 104,
        "type": "SingleChoice",
        "question": "设卷积层有m个特征图,则m由()决定?",
        "options": {
            "A": "上⼀层卷积层的深度",
            "B": "上⼀层卷积核的个数",
            "C": "上⼀层卷积核的深度",
            "D": "m是超参数由程序员⼿⼯设定"
        },
        "answers": [
            "B"
        ],
        "explanation": " 在卷积神经网络中:卷积核的个数 = 下一层数据的深度 = 下一层卷积层卷积核的深度 = 本层的输出通道数 = 提取特征的数量"
    },
    {
        "index": 105,
        "type": "MultipleChoice",
        "question": "以下对于在ILSVRC竞赛中所使⽤的Imagenet数据集描述正确项包括:()。",
        "options": {
            "A": "包含⼤约1400万张图像",
            "B": "包含5万张验证集图像",
            "C": "包含⼤约128万张测试集图像",
            "D": "包含15万张训练集图像",
            "E": "包含1000个类"
        },
        "answers": [
            "B",
            "E"
        ],
        "explanation": "ILSVRC竞赛的分类和检测任务是其核心任务,主要基于ImageNet数据集。Imagenet数据集是一个基于50,000+众包收集的数据集,由李飞飞教授领导,该数据集包含14,197,122张图像,分为21,841个类别。用于组织ILSVRC竞赛的数据是其一个子库,包含1000个类,图像大约包括:1,281,167 张训练集图片(2012年)、50,000 张验证集图片、150,000 张测试集图片 "
    },
    {
        "index": 106,
        "type": "MultipleChoice",
        "question": "在Imagenet⼤规模视觉识别挑战赛中,下列哪些任务没有使⽤Imagenet数据集作为官⽅数据集?( )",
        "options": {
            "A": "图像分类与⽬标定位",
            "B": "⽬标检测",
            "C": "视频⽬标检测",
            "D": "场景分类",
            "E": "场景分割"
        },
        "answers": [
            "D",
            "E"
        ],
        "explanation": "图像分类与目标定位(CLS-LOC)、目标检测(DET)、视频目标检测(VID)、场景分类(Scene Classification):MIT Places2数据集、场景分割(Scene Parsing):MIT ADE20K数据集"
    },
    {
        "index": 107,
        "type": "SingleChoice",
        "question": "最早使⽤卷积神经⽹络CNN在ILSVRC竞赛中获得冠军的模型是()。",
        "options": {
            "A": "LeNet-5",
            "B": "AlexNet",
            "C": "VGGNet",
            "D": "GoogLeNet",
            "E": "ResNet"
        },
        "answers": [
            "B"
        ],
        "explanation": "2012年Alex Krizhevsky, Hinton等人提出了基于卷积神经网络的图像分类,该网络是一个7层的CNN,后来被广泛命名为AlexNet,该模型最终获得了ILSVRC2012分类任务的冠军。 "
    },
    {
        "index": 108,
        "type": "SingleChoice",
        "question": "Imagenet LSVRC竞赛从2010年⾄2017年总共举办了8次,在近三次竞赛中,在分类任务中获得冠军最多的国家(以第⼀作者的国籍为准)是( )。",
        "options": {
            "A": "中国",
            "B": "美国",
            "C": "加拿⼤",
            "D": "英国",
            "E": "德国"
        },
        "answers": [
            "A"
        ],
        "explanation": "在ILSVRC的分类任务中,近三年斩获前几名最多队伍包括:公安三所,新加坡国立大学,牛津大学,香港中文大学,中国科学院,清华大学等,虽然获奖者分布在世界各地,但其第一作者大多都是中国人。点赞!"
    },
    {
        "index": 109,
        "type": "SingleChoice",
        "question": "Imagenet图像数据集始于2009年,该数据集由 ( ) 领导的团队创建。",
        "options": {
            "A": "Alex Krizhevsky 和 Geoffrey Hinton",
            "B": "颜⽔成",
            "C": "汤晓鸥",
            "D": "李飞飞"
        },
        "answers": [
            "D"
        ],
        "explanation": "ImageNet图像数据集始于2009年,当时李飞飞教授等在CVPR2009上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文,之后就是基于ImageNet数据集的8届ILSVRC挑战赛(2010-2017),2017年后,ImageNet由Kaggle继续维护。"
    },
    {
        "index": 110,
        "type": "SingleChoice",
        "question": "( )年,AlexNet在Imagenet LSVRC竞赛中获得冠军,这也带来了深度学习的⼤爆发。",
        "options": {
            "A": "2010",
            "B": "2012",
            "C": "2014",
            "D": "2016"
        },
        "answers": [
            "B"
        ],
        "explanation": "2012年,Alex Krizhevsky、IIya Sutskever在多伦多大学Geoff Hinton的实验室设计出了一个深层的卷积神经网络AlexNet,夺得了2012年ImageNet LSVRC的冠军,且准确率远超第二名(top5错误率为15.3%,第二名为26.2%),引起了举世瞩目的轰动。自2012年AlexNet诞生之后,ImageNet的冠军都被卷积神经网络(CNN)包办了,并且层次越来越深,使得CNN逐渐成为图像识别分类的核心算法模型,也带来了深度学习的大爆发。"
    },
    {
        "index": 111,
        "type": "SingleChoice",
        "question": "以下哪⼀个模型是第⼀个卷积神经⽹络模型?( )",
        "options": {
            "A": "LeNet-5",
            "B": "AlexNet",
            "C": "VGGNet",
            "D": "GoogLeNet",
            "E": "ResNet"
        },
        "answers": [
            "A"
        ],
        "explanation": "LeNet-5最早由Yann LeCun在1989年提出,并在1998年进行改进,该网络是实际上第一个真正的卷积神经网络,并被应用在美国邮政支票的手写字体识别中。"
    },
    {
        "index": 112,
        "type": "MultipleChoice",
        "question": "VGGNet的⼀个重要贡献是证明了堆叠⼩卷积核⽐使⽤⼤卷积核具有更多的优势,这些优势包括()。",
        "options": {
            "A": "可以获得更深的⽹络,即得到更强的⾮线性特性",
            "B": "减少参数,从⽽降低模型对存储的需求",
            "C": "缩⼩特征图尺度,降少计算量",
            "D": "增强神经元的强度,从⽽提⾼神经⽹络的判别能⼒"
        },
        "answers": [
            "A",
            "B"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>相比AlexNet模型,VGG使用了更小的卷积核来生成卷积特征图,主要原因包括:堆叠3个3×3的卷积层(stride 1)具有和7×7的卷积层一样的有效感知域。更强的非线性:从1个卷积层,改为3个卷积层,将获得更深的网络。更少的参数:<span class=\"math inline\">\n &nbsp; &nbsp;3 * (3 &nbsp; &nbsp;<sup>\n &nbsp; &nbsp; 2 &nbsp; &nbsp;</sup>\n &nbsp; &nbsp;<em>\n &nbsp; &nbsp; C &nbsp; &nbsp;</em>\n &nbsp; &nbsp;<sup>\n &nbsp; &nbsp; 2 &nbsp; &nbsp;</sup>\n &nbsp; &nbsp;) &nbsp; &nbsp;<em>\n &nbsp; &nbsp; v &nbsp; &nbsp;</em>\n &nbsp; &nbsp;<em>\n &nbsp; &nbsp; s &nbsp; &nbsp;</em>\n &nbsp; &nbsp;.&nbsp;7 &nbsp; &nbsp;<sup>\n &nbsp; &nbsp; 2 &nbsp; &nbsp;</sup>\n &nbsp; &nbsp;<em>\n &nbsp; &nbsp; C &nbsp; &nbsp;</em>\n &nbsp; &nbsp;<sup>\n &nbsp; &nbsp; 2 &nbsp; &nbsp;</sup>\n &nbsp; </span> &nbsp;for 每层C个通道</p></div>"
    },
    {
        "index": 113,
        "type": "Judgement",
        "question": "在VGGNet中,作者验证了在AlexNet中提出的局部正则响应层()能够有效地提⾼模型的辨别能⼒。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": "作者在VGGNet的B模型中测试了局部正则响应层的作用,然而并没有得到LRN可以提高性能的证据。因此,在之后的模型和其他网络中,LRN倍彻底弃用。\n\n \n  \n\n"
    },
    {
        "index": 114,
        "type": "SingleChoice",
        "question": "VGGNet的成功证明了,对模型进⾏()的增加能够有效提⾼模型的识别能⼒。",
        "options": {
            "A": "宽度",
            "B": "深度",
            "C": "通道数",
            "D": "特征图尺度"
        },
        "answers": [
            "B"
        ],
        "explanation": "VGG最大的贡献是通过组合简单的3×3卷积为VGG块,然后通过堆叠VGG块实现模型深度的增加。 "
    },
    {
        "index": 115,
        "type": "SingleChoice",
        "question": "在基于VGG模型的卷积神经⽹络中,⼤多数的参数主要位于()。",
        "options": {
            "A": "低层的卷积层",
            "B": "⾼层的卷积层",
            "C": "⾼层的全连接层",
            "D": "低层的池化层",
            "E": "⾼层的池化层"
        },
        "answers": [
            "C"
        ],
        "explanation": "在卷积神经网络中,参数最多的部分是全连接层,而全连接层通常位于网络的高层部分,即输出部分。"
    },
    {
        "index": 116,
        "type": "SingleChoice",
        "question": "在基于VGG模型的卷积神经⽹络中,⼤多数的内存主要消耗在()。",
        "options": {
            "A": "低层的卷积层",
            "B": "⾼层的卷积层",
            "C": "⾼层的全连接层",
            "D": "低层的池化层",
            "E": "⾼层的池化层"
        },
        "answers": [
            "A"
        ],
        "explanation": "由于低层特征图的尺度比较大,因此需要较多的内存空间来进行存储。"
    },
    {
        "index": 117,
        "type": "SingleChoice",
        "question": "堆叠3个3×3的卷积层,可以获得与()的卷积层⼀样的感知域。",
        "options": {
            "A": "5×5",
            "B": "7×7",
            "C": "9×9",
            "D": "11×11"
        },
        "answers": [
            "B"
        ],
        "explanation": "1个3×3的卷积核视野为3×3;2个3×3的卷积核堆叠,其视野为5×5;3个3×3的卷积核堆叠,其视野为7×7;4个3×3的卷积核堆叠,其视野为9×9。 "
    },
    {
        "index": 118,
        "type": "SingleChoice",
        "question": "VGGNet中⼩卷积核起到了⾄关重要的作⽤,下列维度的卷积核哪⼀个是标准VGG16中最主要的卷积核。()",
        "options": {
            "A": "[5×5]",
            "B": "[4×4]",
            "C": "[3×3]",
            "D": "[2×2]",
            "E": "[1×1]"
        },
        "answers": [
            "C"
        ],
        "explanation": "[3×3]卷积是VGG最主要的贡献之一,堆叠[3×3]的卷积可以获得和AlexNet中[7×7]卷积核同样的感知域。"
    },
    {
        "index": 119,
        "type": "SingleChoice",
        "question": "以下增加神经⽹络深度的⽅法中,VGG所使⽤的是()。",
        "options": {
            "A": "直接进⾏更多的全连接层的堆叠",
            "B": "直接进⾏更多的卷积层的堆叠",
            "C": "精⼼设计各种功能层的组合",
            "D": "模块化地进⾏卷积层组的堆叠"
        },
        "answers": [
            "D"
        ],
        "explanation": "选项A的设计和深度多层感知机的思想差不多,然而它最大的问题就是参数过多,实现的价值过于昂贵;选项B和D的区别是前者只是单纯地进行卷积层的堆叠,而后者是先组合再按组进行堆叠,单纯的进行卷积层的堆叠会导致超参数的确定困难;选项C所采用的精心设计网络是GoogLeNet的主要方法,这种方法性价比最好,但是设计和实现较为困难;选项D是本题VGG的设计方法。"
    },
    {
        "index": 120,
        "type": "Judgement",
        "question": "VGGNet和GoogLeNet模型的设计都没有超过30层,这有利的证明了深度学习中的“深度”的上限不超过30层。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "false"
        ],
        "explanation": " 虽然VGGNet和GoogLeNet的深度分别为19层和22层,但是这并不是说深度学习已经失效,网络无法设计的更深主要是因为随着深度的增加,梯度逐渐消失了(ResNet中可以找到相关的论述)。随着残差网络的提出,深度学习证明了深度对于模型性能的提升是积极意义的,因为它提供了更多的非线性特性。\n\n \n  \n\n"
    },
    {
        "index": 121,
        "type": "SingleChoice",
        "question": "在GoogLeNet中,前后两个相邻的卷积层组的特征图尺度间的关系是()。",
        "options": {
            "A": "尺度保持不变",
            "B": "尺度缩⼩⼀半",
            "C": "尺度放⼤⼀倍",
            "D": "⽆法确定"
        },
        "answers": [
            "B"
        ],
        "explanation": "在GoogLeNet中,随着深度的增加,每个卷积层组中特征图的尺度都将缩小一半,而通道数则逐渐增加。与GoogLeNet类似,VGG和ResNet也有类似的特性。 "
    },
    {
        "index": 122,
        "type": "SingleChoice",
        "question": "在标准的GoogLeNetV1中总共包含()有权重参数的层。",
        "options": {
            "A": "16",
            "B": "19",
            "C": "22",
            "D": "37"
        },
        "answers": [
            "C"
        ],
        "explanation": "标准的GoogLeNetV1总共有22层。"
    },
    {
        "index": 123,
        "type": "SingleChoice",
        "question": "下列模型中,参数数量最少的模型是()。",
        "options": {
            "A": "AlexNet",
            "B": "VGGNet",
            "C": "GoogLeNet",
            "D": "ResNet"
        },
        "answers": [
            "C"
        ],
        "explanation": "GoogLeNet是一个精心设计的模型,主要对在执行效率方面进行了改进。它的参数比AlexNet少12倍,比VGG16少16倍。"
    },
    {
        "index": 124,
        "type": "Judgement",
        "question": "GoogLeNet使⽤并⾏的思路设计了Inception模块,并联了多个不同感知域的卷积层和池化层,这种⽅法有效组合了不同视野下特征,起到了特征互补的作⽤。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": null
    },
    {
        "index": 125,
        "type": "MultipleChoice",
        "question": "以下对计算复杂度描述正确的公式包括(),其中K表⽰卷积核,F表⽰特征图,in_channel表⽰输⼊通道,out_channel表⽰输出通道,N=Numbers表⽰数量。",
        "options": {
            "A": "参数数量=\nK_width × K_Height × N_in_channels × N_out_channels",
            "B": "参数数量=\nK_width × K_Height × K_Depth × N_Kernels",
            "C": "FLOPS = (K_width × K_Height × K_Depth) × N_Kernels × (F_Width × F_Height × F_Depth)",
            "D": "FLOPS = (K_width × K_Height × K_Depth) × N_Kernels × (F_Width × F_Height)"
        },
        "answers": [
            "A",
            "B",
            "D"
        ],
        "explanation": "选项A和B描述的是同样的概念,其中输入通道数等于卷积核的深度,输出通道数等于卷积核的个数。选项C和D描述的是浮点运算次数,对于每个3D卷积核来说,所有元素在每个空间位置都需要进行一次浮点计算。因此,它所滑动的次数,决定了它浮点运算的次数。当特征图越大的时候,浮点运算次数也越多。 "
    },
    {
        "index": 126,
        "type": "MultipleChoice",
        "question": "在GoogLeNetV1的Inception模块中,融合了以下哪⼏种尺度的卷积层?()",
        "options": {
            "A": "1×1卷积",
            "B": "3×3卷积",
            "C": "5×5卷积",
            "D": "7×7卷积",
            "E": "11×11卷积"
        },
        "answers": [
            "A",
            "B",
            "C"
        ],
        "explanation": "Inception模块的设计思想是兼容并蓄,融合多种不同的功能模块,但它也不是无限制地乱融合。其中1×1卷积用来实现特征通道的调整(大多数时候是减少)和特征图的融合;3×3卷积由于性能较好,被用于提取比较鲁棒的局部特征;5×5卷积用来获得比3×3更多的轮廓信息。而对于7×7和11×11等更大的卷积,作者并没有提及其融合性能。因此,并不能排除它们也具有一定的作用,例如它们也许可以通过更大的感受野来获得比3×3和5×5更多轮廓信息。但这些更大的卷积核也存在一些比较严重的问题,那就是参数过多,这与GoogLeNet和Inception的设计初衷相背离,所以,一般认为这是Inception没有使用更大卷积核的原因。 "
    },
    {
        "index": 127,
        "type": "SingleChoice",
        "question": "以下以卷积为核⼼的功能层/模块中,计算复杂度最⾼()的模块和最低的模块分别是()。",
        "options": {
            "A": "3×3卷积 基本Inception模块",
            "B": "5×5卷积 基本Inception模块",
            "C": "3×3卷积 带瓶颈设计的Inception模块",
            "D": "5×5卷积\n带瓶颈设计的Inception模块"
        },
        "answers": [
            "D"
        ],
        "explanation": "在神经网络中,衡量模型复杂性的参数主要包括两个,一个是参数数量,一个是浮点计算数。虽然它们被用来衡量的是不同的指标,但通常它们也具有一定的相关性。也就是说参数比较多的模型,所需要的浮点运算数量也比较多,反之亦然。在题目的选项中所列出的四种功能层/模块,其参数数量和浮点运算数的大小关系为:5×5卷积 > 3×3卷积 > 基本Inception模块 > 带瓶颈设计的Inception模块。 "
    },
    {
        "index": 128,
        "type": "SingleChoice",
        "question": "在GoogLeNet中,使⽤1×1卷积的瓶颈设计通过减少特征通道的数量,实现了()。",
        "options": {
            "A": "计算复杂性的降低",
            "B": "参数数量的减少",
            "C": "模型判别能⼒的提⾼",
            "D": "模型结构的简化"
        },
        "answers": [
            "A"
        ],
        "explanation": "基于1×1卷积的瓶颈结构(bottleneck)可以保证输出特征图尺度不变的情况下调节特征图的数量。GoogLeNet利用瓶颈结构减少了卷积核的个数,但保留了特征图的尺度,然后在后续的结构中又再次还原了特征图的数量,这种设计通过强制特征融合减了特征图的数量,从而降低了计算复杂性(即减少了计算的次数)。"
    },
    {
        "index": 129,
        "type": "MultipleChoice",
        "question": "以下对神经⽹络模型特性的描述种,符合NiN模型的包括()。",
        "options": {
            "A": "随着⽹络深度的加深,特征图的尺度逐渐变⼤",
            "B": "语义信息越强的层次,特征通道数量越多",
            "C": "交替使⽤NiN⽹络和最⻓为3的最⼤池化层来构建模型",
            "D": "NiN模型使⽤全卷积架构来实现特征提取,因此除最后的Softmax外,只存在卷积层",
            "E": "NiN模型最重要的设计初衷是解决全连接层参数过多的问题"
        },
        "answers": [
            "B",
            "E"
        ],
        "explanation": "NiN模型与VGGNet类似,同样采用模块化的设计思路,它通过组合NiN模块和步长为2的最大池化层来构建模型。在NiN中,随着深度的增加,特征图的尺度逐渐缩小,同时输出通道数逐渐增加。在最后的输出部分,它使用全局平均池化直接获得输出类别数。它是少数彻底摒弃全连接层的神经网络模型。 "
    },
    {
        "index": 130,
        "type": "MultipleChoice",
        "question": "在NiN⽹络中,使⽤了以下哪些功能层?()",
        "options": {
            "A": "平均池化层(Mean-Pooling)",
            "B": "最⼤池化层(Max-Pooling\nLayer)",
            "C": "卷积层(Convolution\nLayer)",
            "D": "全局平均池化层(Global\nAverage Pooling Layer)",
            "E": "全连接层(Fully-Connection Layer)"
        },
        "answers": [
            "B",
            "C",
            "D"
        ],
        "explanation": "NiN网络最大的贡献是利用1×1卷积来实现参数的大幅缩小,为了做得足够极致,它还去掉了最后的全连接层,甚至连映射类别标签的全连接层也没有保留。为了实现分类,它引入了一种新的功能层,即全局平均池化(Global Avg Pooling),该层最主要的功能是实现对最后一个特征图的融合,并输出成类别标签。此外,它使用NiN模块和Max-Pooling层进行交替组合,实现网络主干的构建。"
    },
    {
        "index": 131,
        "type": "MultipleChoice",
        "question": "在NiN⽹络中,1×1卷积的功能包括()。",
        "options": {
            "A": "增加模型的⾮线性性",
            "B": "提供更多种类的特征,即增加特征的多样性",
            "C": "对单通道特征进⾏融合,⽣成复合特征",
            "D": "减少⽹络的参数数量"
        },
        "answers": [
            "A",
            "C",
            "D"
        ],
        "explanation": "在NiN网络中,1×1卷积设计的初衷是通过减少网络参数数量来降低过拟合的风险,并减少模型对硬件资源的消耗。因为更小的卷积,将带来更多的参数共享特性。从另外一个角度来看,1×1卷积因为使用的窗口尺度仅仅只有一个像素,所以它能够实现对一个局部区域的特征融合。并且,当它扫过整个特征图时,它的输出的尺度也将和输入尺度保持不变,这就相当于在不改变尺度的同时,还能够增加网络的深度。此外,由于卷积核的数量是一个超参数,所以,可以人为地进行输出通道(即卷积核数量/深度)的设置,从而起到调节特征图深度的目的。"
    },
    {
        "index": 132,
        "type": "MultipleChoice",
        "question": "在卷积神经⽹络中,⼤多数参数来源于()。",
        "options": {
            "A": "卷积层",
            "B": "卷积层与池化层之间",
            "C": "卷积层与全连接层的交接处",
            "D": "全连接层"
        },
        "answers": [
            "C",
            "D"
        ],
        "explanation": "对于卷积神经网络来说,大多数参数来源于全连接层以及全连接层和卷积层的边界。其中全连接层因为密集连接所以导致参数较多;而交界处主要是因为卷积层的输入通道数和输出通道数都较多,也导致参数较多。并且,相对来说,交界处的参数比全连接层还要更多一些。"
    },
    {
        "index": 133,
        "type": "MultipleChoice",
        "question": "下列模型中,哪些模型利⽤1×1卷积来实现瓶颈设计。()",
        "options": {
            "A": "AlexNet",
            "B": "VGGNet",
            "C": "GoogLeNet",
            "D": "ResNet"
        },
        "answers": [
            "C",
            "D"
        ],
        "explanation": "<div class=\"custom_ueditor_cn_body\"><p>GoogLeNet和ResNet</p></div>"
    },
    {
        "index": 134,
        "type": "MultipleChoice",
        "question": "观察下列结构，哪些属于残差模块。",
        "options": {
            "A": "",
            "B": "",
            "C": "",
            "D": ""
        },
        "answers": [
            "B",
            "A"
        ],
        "explanation": null
    },
    {
        "index": 135,
        "type": "SingleChoice",
        "question": "在残差结构公式 H() = F() + x 中,表⽰残差⼀项是()。",
        "options": {
            "A": "F(x)",
            "B": "H(x)",
            "C": "x",
            "D": "F(x)+x"
        },
        "answers": [
            "A"
        ],
        "explanation": "在残差结构公式 H(x) = F(x) + x 中,H(x)=x 表示恒等映射,其中 x 是输入,H(x)是恒等映射的结果,F(x) 是残差。 "
    },
    {
        "index": 136,
        "type": "SingleChoice",
        "question": "以下科学家,哪⼀位是ResNet的主要设计者?()",
        "options": {
            "A": "Alex Krizhevsky",
            "B": "Christian Szegedy",
            "C": "Karen Simonyan",
            "D": "Kaiming He"
        },
        "answers": [
            "D"
        ],
        "explanation": "Alex Krizhevsky是AlexNet的设计者,Christian Szegedy是GoogLeNet的缔造者,Laren Simonyan是VGGNet的设计者,Kaiming He(何凯明)是ResNet的缔造者。"
    },
    {
        "index": 137,
        "type": "SingleChoice",
        "question": "下列模型中,哪⼀个模型证明了深度学习的“深度”并没有失效,并且将深度学习真正推到超过1000层的深度。()",
        "options": {
            "A": "AlexNet",
            "B": "VGGNet",
            "C": "GoogLeNet",
            "D": "ResNet"
        },
        "answers": [
            "D"
        ],
        "explanation": "借助于残差结构和恒等关系的设计,ResNet证明了深度对于卷积神经网络是有积极意义的,并且在Cifar10数据集上实现了超过1000层的深度设计。"
    },
    {
        "index": 138,
        "type": "SingleChoice",
        "question": "深度模型的深度\"失效\"的主要原因是()。",
        "options": {
            "A": "过拟合",
            "B": "欠拟合",
            "C": "优化困难",
            "D": "更深的模型意义不⼤"
        },
        "answers": [
            "C"
        ],
        "explanation": "随着深度模型深度的增加,性能出现了反转,即更深的模型性能反而不如较浅的深度模型,但是这个过程中并没有出现过拟合的问题,因此更多的可能是更难以被优化。"
    },
    {
        "index": 139,
        "type": "Judgement",
        "question": "深度学习⽐浅层模型具有更强的性能,要训练⼀个性能强劲的卷积神经⽹络,⼀般通常要求有⼤量的数据。即⼤数据是深度学习的基本保证。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": "我们一般认为深度学习的发展得益于三点:计算机性能的提高(GPU并行)、算法的改进及大数据时代的来临。大数据是训练一个良好的深度模型的基本保证。当然,在很多时候,我们不一定能获得大量的数据,因此迁移学习算法、小数据训练方法等方法吸引了很多的研究者。但是,足够的、丰富的数据依然是模型良好性能的保证。\n\n \n  \n\n"
    },
    {
        "index": 140,
        "type": "SingleChoice",
        "question": "()在进⾏微调训练时,如果许需要冻结整个卷积部分,仅仅只微调线性分类器,这意味着⽬标数据集是⼀个(),并且⽬标数据集和原始数据集()。",
        "options": {
            "A": "⼩数据集\n⾮常相似",
            "B": "⼩数据集 ⾮常不同",
            "C": "⼤数据集 ⾮常相似",
            "D": "⼤数据集 ⾮常不同"
        },
        "answers": [
            "A"
        ],
        "explanation": "在进行微调训练时,如果许需要冻结整个卷积部分,仅仅只微调线性分类器,这意味着目标数据集是一个小数据集,并且目标数据集和原始数据集非常相似。"
    },
    {
        "index": 141,
        "type": "SingleChoice",
        "question": "如果⽬标数据集⾮常⼤,并且与源数据集不同,在选择微调策略的时候,通常应该()。",
        "options": {
            "A": "冻结整个卷积部分,仅微调线性分类器",
            "B": "冻结开头的部分卷积层,微调⼤部分卷积层和线性分类器",
            "C": "仅微调最后的⼩部分卷积层和线性分类器",
            "D": "微调整个模型"
        },
        "answers": [
            "D"
        ],
        "explanation": "小数据集,且与预训练模型数据集相似:冻结整个卷积部分,仅微调分类器。小数据集,且与预训练模型数据集不同:冻结顶部的少数卷积层,并训练其他卷积层和分类器。大数据集,且与预训练模型数据集相似:冻结大部分卷积层,仅微调少数卷积层和分类器。大数据集,且与预训练模型数据集不同:微调整个模型的所有层。"
    },
    {
        "index": 142,
        "type": "SingleChoice",
        "question": "()在⼀个深度模型中,越接近输⼊层的特征越(-),越接近输出层的特征越(-)。",
        "options": {
            "A": "通⽤ 通⽤",
            "B": "通⽤\n特别",
            "C": "特别 特别",
            "D": "特别 通⽤"
        },
        "answers": [
            "B"
        ],
        "explanation": "在一个深度模型中,越接近输入层的特征越通用,越接近输出层的特征越特别;并且越接近输入层(低层)的特征越局部,越接近输出层(高层)的特征越全局。"
    },
    {
        "index": 143,
        "type": "SingleChoice",
        "question": "()迁移学习是指将(-)领域学习获得的知识迁移到(-)领域,以确保最终能获得更好的学习效果。",
        "options": {
            "A": "源 源",
            "B": "源\n⽬标",
            "C": "⽬标 源",
            "D": "⽬标 ⽬标"
        },
        "answers": [
            "B"
        ],
        "explanation": "把一个领域(源领域,Source Domain)的知识迁移到另一个领域(目标领域,Target Domain),使得目标领域能够获得更好的学习效果。"
    },
    {
        "index": 144,
        "type": "SingleChoice",
        "question": "当⽬标数据集规模较⼩,且与预训练模型数据较为相似,则微调策略为()。",
        "options": {
            "A": "冻结整个卷积部分,仅微调分类器。",
            "B": "冻结顶部的少数卷积层,并训练其他卷积层和分类器。",
            "C": "冻结⼤部分卷积层,仅微调少数卷积层和分类器。",
            "D": "微调整个模型的所有层。"
        },
        "answers": [
            "A"
        ],
        "explanation": "小数据集,且与预训练模型数据集相似:冻结整个卷积部分,仅微调分类器。小数据集,且与预训练模型数据集不同:冻结顶部的少数卷积层,并训练其他卷积层和分类器。大数据集,且与预训练模型数据集相似:冻结大部分卷积层,仅微调少数卷积层和分类器。大数据集,且与预训练模型数据集不同:微调整个模型的所有层。"
    },
    {
        "index": 145,
        "type": "Judgement",
        "question": "迁移学习通常使⽤预训练模型来作为新任务模型的权重起始值,⽽不再使⽤随机初始化等⽅法来进⾏权重初始化。()",
        "options": {
            "true": "正确",
            "false": "错误"
        },
        "answers": [
            "true"
        ],
        "explanation": "迁移学习的具体应用就是使用预训练好的模型(源任务)的权重,作为目标任务的权重初始值,从而使目标任务能够以更好的初始状态开始学习。从而提高训练模型的收敛速度和收敛结果。\n\n \n  \n\n"
    },
    {
        "index": 146,
        "type": "MultipleChoice",
        "question": "以下任务可以使⽤迁移学习来改进计算效率的包括()。",
        "options": {
            "A": "分类任务(Classification)",
            "B": "⽬标检测(Detection)",
            "C": "视频加字幕(Captioning)",
            "D": "基于视觉内容的问答(VQA)",
            "E": "图像分割(Segmentation)"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D",
            "E"
        ],
        "explanation": "基本上所有的计算机视觉任务都可以使用迁移学习方法来改进计算效率。 "
    },
    {
        "index": 147,
        "type": "MultipleChoice",
        "question": "常⻅的预训练模型包括()。",
        "options": {
            "A": "AlexNet",
            "B": "VGGNet",
            "C": "GoogLeNet",
            "D": "ResNet"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "以上ABCD均为常见的预训练模型,并且大多基于Imagenet数据集进行预训练。"
    },
    {
        "index": 148,
        "type": "MultipleChoice",
        "question": "以下属于迁移学习分类任务的包括()。",
        "options": {
            "A": "特征迁移",
            "B": "样本迁移",
            "C": "参数迁移",
            "D": "关系知识迁移"
        },
        "answers": [
            "A",
            "B",
            "C",
            "D"
        ],
        "explanation": "特征迁移: 通过源领特征表示,把知域学习一个好的识通过特征的形式进行编码,并从源领域传递到目标领域,提升其任务效果。样本迁移: 源领域中数据的某一部分可以通过调整权重的方法重用,用于目标领域的学习。参数迁移: 任务之间共享相同的模型参数或者是服从相同的先验分布。关系知识迁移: 相关领域之间的知识迁移。"
    },
    {
        "index": 149,
        "type": "MultipleChoice",
        "question": "以下属于迁移学习的优点是包括哪些()。",
        "options": {
            "A": "节约成本,特别是⽤于标注数据的成本",
            "B": "可以使⽤⼩数据集学到更鲁棒的特征,从⽽解决原本需要⼤数据集才能完成的任务",
            "C": "提⾼训练速度",
            "D": "提⾼测试速度"
        },
        "answers": [
            "A",
            "B",
            "C"
        ],
        "explanation": "迁移学习的应用主要可以获得三个优势。第一,通过对已有知识的使用,实现利用更少的样本完成训练;第二,因为从预训练模型开始训练,因此有更好的初始化值,这大大提高了训练的收敛速度;第三,因为有更好的初始化值,可以使模型更容易收敛到更好的最优值。"
    },
    {
        "index": 150,
        "type": "MultipleChoice",
        "question": "以下哪些学习⽬标适合使⽤迁移学习算法来改进模型?()",
        "options": {
            "A": "源任务是学习识别猫狗,⽬标任务是学习识别⽼虎⼤象",
            "B": "源任务是⼿写字体识别,⽬标任务是对⾏⼈⾝份进⾏确认",
            "C": "源任务是区分场景归属哪⼀个城市,⽬标任务是对妆容进⾏⾃动美化",
            "D": "源任务是识别⼀⾸歌是谁唱的,⽬标任务是识别⼀个电影的主演是谁"
        },
        "answers": [
            "A",
            "C"
        ],
        "explanation": "是否适合使用迁移学习算法来改进模型,主要看源任务和目标任务的相关性是否足够强,或者说源任务的知识是否对目标任务有帮助。在A选项中目标都属于动物,显然之间的关联性是很强的;C选项的城市识别和美妆表面上看上去是不相关的,但是其本质都是像素级的色彩相关性,所以依然可用;B选项手写字体虽然也属于像素表达,但是更看重的是笔画间的相关性,和行人识别差异较大,所以迁移学习的效果通常不会很好;D选项从语音识别到视觉识别差异较大,通常不适合进行知识迁移。"
    }
]